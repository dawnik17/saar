{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-09-06 08:33:33.754794: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 08:33:35.293235: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-06 08:33:35.293398: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-06 08:33:35.293415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, \n",
    "                          AutoTokenizer, \n",
    "                          GenerationConfig, \n",
    "                          TrainingArguments, \n",
    "                          Trainer)\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,0,3\"  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeftModel:\n",
    "    @staticmethod\n",
    "    def load_base_model(model_path=\"google/flan-t5-base\"):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.bfloat16, device_map='auto'\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_peft_adapter(\n",
    "        base_model_path, peft_model_path, train=False, merge_adapter=True\n",
    "    ):\n",
    "        model, tokenizer = self.load_base_model(base_model_path)\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model, peft_model_path, torch_dtype=torch.bfloat16, is_trainable=train, device_map='auto')\n",
    "\n",
    "        if merge_adapter:\n",
    "            model = model.merge_and_unload()\n",
    "\n",
    "            if train:\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # merge the adapter to the main model\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def save_peft_adapter(model, model_path):\n",
    "        model.save_pretrained(model_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_peft_and_save(model, model_path):\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(model_path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def save_tokenizer(tokenizer):\n",
    "        tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original model\n",
    "name='google/flan-t5-base'\n",
    "model, tokenizer = PeftModel.load_base_model(model_path=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 - Load Dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inshorts_scraped.json\n",
      "git_data.json\n"
     ]
    }
   ],
   "source": [
    "# load and aggregate raw data\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Specify the folder path containing the JSON files\n",
    "folder_path = './data'\n",
    "\n",
    "# Initialize an empty list to aggregate the data\n",
    "data = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        print(filename)\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read and parse JSON data from the file\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            file_data = json.load(json_file)\n",
    "            \n",
    "            # Assuming each JSON file contains a list of dictionaries\n",
    "            if isinstance(file_data, list):\n",
    "                data.extend(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# curate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602641"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "data = [news for news in data if news[\"full_text\"] != \"\" and \"JavaScript is not available\" not in news[\"full_text\"] and \"reuters\" not in news[\"link\"]]\n",
    "random.shuffle(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for news in data:\n",
    "    if \"<p>\" in news[\"summary\"]:\n",
    "        # Regular expression to match content between <p> tags\n",
    "        pattern = re.compile(r'<p>(.*?)</p>', re.DOTALL)\n",
    "        matches = pattern.findall(news[\"summary\"])\n",
    "\n",
    "        # Extracted content from <p> tags\n",
    "        extracted_content = [re.sub(r'<.*?>', '', match) for match in matches]\n",
    "        news[\"summary\"] = max(extracted_content, key=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for news in data:   \n",
    "            input_prompt, label = self._get_summary_prompt(news)\n",
    "            self.inputs.append(input_prompt)\n",
    "            self.labels.append(label)\n",
    "            \n",
    "            # input_prompt, label = self._get_title_prompt(news)\n",
    "            # self.inputs.append(input_prompt)\n",
    "            # self.labels.append(label)\n",
    "            \n",
    "        \"\"\"\n",
    "        Combine the lists using zip\n",
    "        Shuffle the combined list\n",
    "        Unpack the shuffled pairs back into separate lists\n",
    "        And then tokenize\n",
    "        \"\"\"\n",
    "        combined = list(zip(self.inputs, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.inputs, self.labels = zip(*combined)\n",
    "\n",
    "        # tokenize\n",
    "        self.inputs = tokenizer(self.inputs, \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\").input_ids\n",
    "\n",
    "        self.labels = tokenizer(self.labels, \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\").input_ids\n",
    "            \n",
    "    def __len__(self): \n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_summary_prompt(example):\n",
    "        # word count round off\n",
    "        multiple = 25\n",
    "        word_count = len(example[\"summary\"].split())\n",
    "        word_count = int(round(word_count / multiple)) * multiple\n",
    "        \n",
    "        start_prompt = f'Summarize this news article in {word_count} words.\\n\\n'\n",
    "        end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "        prompt = start_prompt + example[\"full_text\"] + end_prompt\n",
    "\n",
    "        return prompt, example[\"summary\"]\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _get_title_prompt(example):\n",
    "#         # word count round off\n",
    "#         multiple = 5\n",
    "#         word_count = len(example[\"title\"].split())\n",
    "#         word_count = int(ceil(word_count / multiple)) * multiple\n",
    "        \n",
    "#         start_prompt = f'Give a title to the given news article in not more than {word_count} words.\\n\\n'\n",
    "#         mid_prompt = '\\n\\nSummary: '\n",
    "#         end_prompt = '\\n\\nTitle: '\n",
    "\n",
    "#         prompt = start_prompt + example[\"full_text\"] + mid_prompt + example[\"summary\"] + end_prompt\n",
    "#         return prompt, example[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TextDataset(data, tokenizer)\n",
    "# test_data = TextDataset(data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([12198,  1635,  1737,    48,  1506,  1108,    16,   944,  1234,     5,\n",
       "            96,  3713,  3392,    19,    69,     7,   233,    11,    27,  5712,\n",
       "            25,  6224,   976,     3,    88,   243,    16,     3,     9,  1424,\n",
       "          1115,     5, 16706, 16054,     7,    16,     8,    36,  2452,  5402,\n",
       "          1511,    13,  2415,     7,  2256,   497,    79,    33,   365,  2437,\n",
       "         26877,   297,    45,   216,   172,  4243,   521,   107,  4719,  2366,\n",
       "             5,    37,  1511,    19,   885,    12,     8,   312,  3478,    15,\n",
       "             7,    15,  4947,     6,     3,     9, 13389,    21,   321,     8,\n",
       "           789,    11, 16054,     7,    12,   129,  7749,     5,    86,     8,\n",
       "          5023,    45,    46,    64,   159, 16221,    26,  1128,     6,  1363,\n",
       "          4498,    52, 30157,   243,     3,    99,  3068,    29,    23, 10172,\n",
       "           343,     7,   808,   147,    16, 11380,     6,    79,   133,  7663,\n",
       "             3,     9,  5888,    12,     8,  1297,   312,  3478,    15,     7,\n",
       "            15,  2074,     3,    18,  4804,     9,    11,  3068,    29,    23,\n",
       "         16932,     6,    38,   168,    38, 11337,     5,   216,   243,   112,\n",
       "          2426,   228,   470,    36,  7901,    15,    26,    28, 16706, 16054,\n",
       "             7,   113,     6,    16,   112,   903,     6,   130,  3510,    57,\n",
       "             8,   907,  1323,    11,  3352,     5,   216,   172,  4243,   521,\n",
       "           107, 20593,     7,  1659,   139, 11380,  4129,   531,  1847,     7,\n",
       "            13,   216,   172,  4243,   521,   107, 20026,     7,    33,   243,\n",
       "            12,    43,   118,  4792,  6237,  5815, 16706, 12673,    16,  2415,\n",
       "             7,  2256,   437,   957,   932,     6,   116,   789,  3859,  3759,\n",
       "            46, 12130,    12, 22591,  2693,     8, 16054,    18, 14796,  1511,\n",
       "             5,  2506,   471,     6,   837,  7471,    13,  1015,  1079, 24967,\n",
       "           243,  2909,    13,   216,   172,  4243,   521,   107, 14248,     7,\n",
       "           130, 12932,  4019,    12,     8,  4756,    16, 11380,     5,   216,\n",
       "           974,    24,  7449,    47,  8609,  3956,   216,   172,  4243,   521,\n",
       "           107,    31,     7,  9683,     3,    18,     3,     9,  1988, 11958,\n",
       "            57, 31238,     5,  7449,    11,   216,   172,  4243,   521,   107,\n",
       "            33, 24448,  4804,     9,     6,   298,  1363,     3, 27409,    31,\n",
       "             7,   901,     9,  7820,    15,  4220,    17,    19,    46,   326,\n",
       "          5630,    32,    17,    13,  4804,     9, 10172,     5,    37,   471,\n",
       "            18,  2961,  6237,    16,  2415,     7,  2256,  9608,  3676,   778,\n",
       "            30,  1856,     6,   116, 19053,  2196,  2437, 26877,  4128,     6,\n",
       "           379,   192,  1591,    18,   235,    18,  9232, 16953,     7,    11,\n",
       "            46,   799,  6585,    38,   168,    38,   768,  7613,    63,    11,\n",
       "         15721,  1472,     5, 16706,   538,   783,   243,     8,  9102,   141,\n",
       "          3759,     3,     9,   386,    18,  1409,    29,  5402, 12130,    16,\n",
       "             8,  3457,     6,  2050,    11,  3414,    13,  2415,     7,  2256,\n",
       "             6,    11,    47,   492,   600, 15895,   227,    96, 10824,    53,\n",
       "           508,  2302,   121,    13, 14248,     7,     5,  2415,     7,  2256,\n",
       "            19,   359,    21,     8, 16706,   789,   250,    34,  2416,     8,\n",
       "          1784,     6, 10939,     9,     7,  1071,     7,     6,    28,     8,\n",
       "           901,     9,  7820,    15,   842,    40,   232,    30,     8, 15481,\n",
       "          4939,     5,   611,     6,  2314,   783,   263,   150,  2652,    13,\n",
       "             8,   294,  1944,    57,   216,   172,  4243,   521,   107,     5,\n",
       "            37,   312,  3478,    15,     7,    15,   563,    19,    92,   801,\n",
       "            12,    43,  1513,     3,     9,   381,    13, 14248,     7,    16,\n",
       "          2415,     7,  2256,     6,  9005,    53,   312,  3478,    15,     7,\n",
       "            15,  1661,  9411,  1923,  4460,   348,    12, 17640,     8,  4804,\n",
       "             9, 31428,   581,   652,    96, 12247,  5402,   323,    16,     8,\n",
       "             3,     1]),\n",
       " tensor([   37,  2488,    13,     8,   312,  3478,    15,     7,    15,  4804,\n",
       "             9, 20026,   216,   172,  4243,   521,   107,  2426,     6,  4498,\n",
       "             7,   152, 10571,    52, 30157,     6,    65, 11130,   112, 11172,\n",
       "            79,    56, 23990,    16, 11380,     6,   213,    79,    33, 16057,\n",
       "          1661,  6653,  3272,   491,    18, 27409,     5,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    \n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    \n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FULL MODEL TRAINING\n",
    "# EPOCH = 1\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#                                   save_steps=5000,\n",
    "#                                   warmup_steps=10,\n",
    "#                                   logging_steps=100,\n",
    "#                                   weight_decay=0.01,\n",
    "#                                   num_train_epochs=EPOCH,\n",
    "#                                   logging_dir='./logs',\n",
    "#                                   output_dir='./checkpoint',\n",
    "#                                   per_device_eval_batch_size=32,\n",
    "#                                   per_device_train_batch_size=32)\n",
    "\n",
    "# Trainer(model=model,\n",
    "#         args=training_args,\n",
    "#         eval_dataset=test_data,\n",
    "#         train_dataset=train_data,\n",
    "#         data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]), \n",
    "#                                     'labels': torch.stack([f[1] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 7077888\n",
      "all model parameters: 254655744\n",
      "percentage of trainable model parameters: 2.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='225993' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    70/225993 00:31 < 29:01:02, 2.16 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PEFT MODEL TRAINING\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "EPOCH = 3\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))\n",
    "\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "                                  # save_steps=5000,\n",
    "                                  save_strategy=\"no\",\n",
    "                                  warmup_steps=10,\n",
    "                                  logging_steps=5000,\n",
    "                                  weight_decay=0.01,\n",
    "                                  num_train_epochs=EPOCH,\n",
    "                                  logging_dir='./logs',\n",
    "                                  output_dir='./checkpoint',\n",
    "                                  learning_rate=0.0001,\n",
    "                                  auto_find_batch_size=True)\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "                model=peft_model,\n",
    "                args=peft_training_args,\n",
    "                train_dataset=train_data,\n",
    "                data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]), \n",
    "                                            'labels': torch.stack([f[1] for f in data])})\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save peft adapter\n",
    "adapter_path = \"./checkpoint/adapter/\"\n",
    "PeftModel.save_peft_adapter(model=peft_model, model_path=adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge peft with main model\n",
    "# and save the model\n",
    "model_path = \"./checkpoint/\"\n",
    "PeftModel.merge_peft_and_save(model=peft_model, tokenizer=tokenizer, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pretrained flan t5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original model\n",
    "model_name='google/flan-t5-base'\n",
    "original_model, original_tokenizer = PeftModel.load_base_model(model_path=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the saved peft inshorts model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./checkpoint/\"\n",
    "peft_model, tokenizer = PeftModel.load_base_model(model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "FULL TEXT:\n",
      "Days after Rajyavardhan Singh Rathore said that Sonia Gandhi and Rahul Gandhi should be tried for “treason”, the Congress hit back at the Bharatiya Janata Party MP for speaking “blatant lies”, claiming that during the 2008 Beijing Olympics, not only did the Congress Parliamentary party chief visit the Games Village but also met with Indian athletes in the Indian block. UPA chairperson Sonia Gandhi with party leader Rahul Gandhi. (ANI file) {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} Speaking in the Lok Sabha on Thursday during a no-confidence motion against the NDA government, Rathore had claimed that Sonia Gandhi and Rahul Gandhi met the Communist Party of China in 2008 when he was in Beijing during the Olympics. “I was at the 2008 Beijing Olympics (in China). We came to know that Sonia Gandhi and Rahul Gandhi are coming to meet us. They didn't come to meet us. They met the Communist Party of China. They should be tried for treason,” Rathore said. Later, Rathore said, “Sonia Gandhi and Rahul Gandhi came to Beijing in 2008. I was there at that time. Their car stopped on the road for 2 minutes and they went away from there. And why did I say 'treason' in Parliament? Because they were in China to sign a secret deal with the Communist Party of China. They did not go there as government representatives. The nation wants to know what was the secret deal.” {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} Reacting to Rathore, who won the silver at the 2004 Athens Olympics for double trap shooting, Congress spokesperson Supriya Shrinate wrote in Hindi on X (formally Twitter, “Shame on you Rajyavardhan Rathore that you were once a part of the Indian Army… During the 2008 Beijing Olympics, not only did Sonia Gandhi visit the Games Village, but she also met with Indian athletes in the Indian block. This has been confirmed by boxer Vijender Singh himself and mentioned by Abhinav Bindra in his book ‘A Shot At History’. Abhinav Bindra won a gold medal in the 2008 Olympics, while Vijender Singh won a bronze medal.” {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} “This reflects the influence of your company. You belong to a party where even the prime minister and the home minister are experts in lying,” she added. Manipur viral video incident Raking up the Manipur viral video incident, the Congress leader asked Rathore, a former Indian Army colonel, “You were a part of the Kargil War, but when a heinous crime occurred in Manipur with a warrior's wife, you remained silent. What kind of ally would someone be who didn't stand with their soldier?” A video showing two women being paraded naked and molested in Manipur's Kangpokpi had surfaced last month and sparked massive outrage. The husband of one of the women seen in the video is an ex-Army man who also fought in the Kargil War. He had served in the Indian Army as a Subedar of the Assam Regiment. {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} “If you are fond of asking questions on China, then someday you will ask Modi ji why he gave a clean chit to China and took revenge for the martyrs of Galwan by increasing trade with China? Remember, in this battle between traitors and patriots, you stand with the traitors,” she added. The last two days of the motion witnessed a fierce battle between the ruling and the Opposition coalitions over the Manipur violence and other raging issues. On Sunday, Rathore alleged that the root cause of the ethnic conflicts in the northeast is the Congress's policies when they governed the country. \"The root cause of the racial violence in the northeast is Congress's policies. The Congress itself knows this. The Congress is the reason for the terrorism that has been going on inside Kashmir for so many years. The Congress has done the work of dividing this country by caste, sects and languages. The Congress has put the country in the pit by keeping one family at the top,\" the BJP MP said in Jaipur. {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} SHARE THIS ARTICLE ON\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Reacting to BJP MP Rajyavardhan Singh Rathore's allegation that Congress leaders Sonia and Rahul Gandhi met the Communist Party of China during the 2008 Beijing Olympics, Congress spokesperson Supriya Shrinate said that shame on Rathore that he was once a part of the \"Indian Army\". She added, \"[Sonia Gandhi] also met Indian athletes…This has been confirmed by boxer Vijender Singh.\"\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "The Congress has slammed the Bharatiya Janata Party MP for speaking blatantly about the 2008 Beijing Olympics.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: Congress MP Rajyavardhan Singh Rathore said that Sonia Gandhi and Rahul Gandhi met the Communist Party of China in Beijing in 2008. \"They didn't come to meet us. They met the Communist Party of China. They should be tried for treason,\" Rathore added. \"They did not go there as government representatives,\" Rathore added.\n",
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.25316455696202533, 'rouge2': 0.07792207792207792, 'rougeL': 0.17721518987341772, 'rougeLsum': 0.17721518987341772}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.4482758620689655, 'rouge2': 0.2280701754385965, 'rougeL': 0.3103448275862069, 'rougeLsum': 0.3103448275862069}\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "index = 5670\n",
    "news = data[index]\n",
    "full_text = news['full_text']\n",
    "baseline_human_summary = news['summary']\n",
    "\n",
    "# word count round off\n",
    "multiple = 25\n",
    "word_count = len(baseline_human_summary.split())\n",
    "word_count = int(round(word_count / multiple)) * multiple\n",
    "\n",
    "start_prompt = f'Summarize this news article in {word_count} words.\\n\\n'\n",
    "end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "prompt = start_prompt + full_text + end_prompt\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = original_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=4))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'FULL TEXT:\\n{full_text}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')\n",
    "\n",
    "\n",
    "# EVALUATE\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=[original_model_text_output],\n",
    "    references=[baseline_human_summary],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=[peft_model_text_output],\n",
    "    references=[baseline_human_summary],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "FULL TEXT:\n",
      "Days after Rajyavardhan Singh Rathore said that Sonia Gandhi and Rahul Gandhi should be tried for “treason”, the Congress hit back at the Bharatiya Janata Party MP for speaking “blatant lies”, claiming that during the 2008 Beijing Olympics, not only did the Congress Parliamentary party chief visit the Games Village but also met with Indian athletes in the Indian block. UPA chairperson Sonia Gandhi with party leader Rahul Gandhi. (ANI file) {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} Speaking in the Lok Sabha on Thursday during a no-confidence motion against the NDA government, Rathore had claimed that Sonia Gandhi and Rahul Gandhi met the Communist Party of China in 2008 when he was in Beijing during the Olympics. “I was at the 2008 Beijing Olympics (in China). We came to know that Sonia Gandhi and Rahul Gandhi are coming to meet us. They didn't come to meet us. They met the Communist Party of China. They should be tried for treason,” Rathore said. Later, Rathore said, “Sonia Gandhi and Rahul Gandhi came to Beijing in 2008. I was there at that time. Their car stopped on the road for 2 minutes and they went away from there. And why did I say 'treason' in Parliament? Because they were in China to sign a secret deal with the Communist Party of China. They did not go there as government representatives. The nation wants to know what was the secret deal.” {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} Reacting to Rathore, who won the silver at the 2004 Athens Olympics for double trap shooting, Congress spokesperson Supriya Shrinate wrote in Hindi on X (formally Twitter, “Shame on you Rajyavardhan Rathore that you were once a part of the Indian Army… During the 2008 Beijing Olympics, not only did Sonia Gandhi visit the Games Village, but she also met with Indian athletes in the Indian block. This has been confirmed by boxer Vijender Singh himself and mentioned by Abhinav Bindra in his book ‘A Shot At History’. Abhinav Bindra won a gold medal in the 2008 Olympics, while Vijender Singh won a bronze medal.” {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} “This reflects the influence of your company. You belong to a party where even the prime minister and the home minister are experts in lying,” she added. Manipur viral video incident Raking up the Manipur viral video incident, the Congress leader asked Rathore, a former Indian Army colonel, “You were a part of the Kargil War, but when a heinous crime occurred in Manipur with a warrior's wife, you remained silent. What kind of ally would someone be who didn't stand with their soldier?” A video showing two women being paraded naked and molested in Manipur's Kangpokpi had surfaced last month and sparked massive outrage. The husband of one of the women seen in the video is an ex-Army man who also fought in the Kargil War. He had served in the Indian Army as a Subedar of the Assam Regiment. {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} “If you are fond of asking questions on China, then someday you will ask Modi ji why he gave a clean chit to China and took revenge for the martyrs of Galwan by increasing trade with China? Remember, in this battle between traitors and patriots, you stand with the traitors,” she added. The last two days of the motion witnessed a fierce battle between the ruling and the Opposition coalitions over the Manipur violence and other raging issues. On Sunday, Rathore alleged that the root cause of the ethnic conflicts in the northeast is the Congress's policies when they governed the country. \"The root cause of the racial violence in the northeast is Congress's policies. The Congress itself knows this. The Congress is the reason for the terrorism that has been going on inside Kashmir for so many years. The Congress has done the work of dividing this country by caste, sects and languages. The Congress has put the country in the pit by keeping one family at the top,\" the BJP MP said in Jaipur. {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} SHARE THIS ARTICLE ON\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Shameful that Rathore was in Army: INC on Beijing Olympics claim\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Congress MP Slams Rathore for 'blatant lies'\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: Sonia met Indian athletes in Beijing: Congress on Rathore's 'blatant lies'\n",
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.1111111111111111, 'rouge2': 0.0, 'rougeL': 0.1111111111111111, 'rougeLsum': 0.1111111111111111}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.34782608695652173, 'rouge2': 0.0, 'rougeL': 0.17391304347826086, 'rougeLsum': 0.17391304347826086}\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "index = 5670\n",
    "news = data[index]\n",
    "full_text = news['full_text']\n",
    "baseline_human_summary = news['title']\n",
    "\n",
    "# word count round off\n",
    "multiple = 5\n",
    "word_count = len(news[\"title\"].split())\n",
    "word_count = int(ceil(word_count / multiple)) * multiple\n",
    "\n",
    "start_prompt = f'Give a title to the given news article in not more than {word_count} words.\\n\\n'\n",
    "mid_prompt = '\\n\\nSummary: '\n",
    "end_prompt = '\\n\\nTitle: '\n",
    "\n",
    "prompt = start_prompt + news[\"full_text\"] + mid_prompt + news[\"summary\"] + end_prompt\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = original_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'FULL TEXT:\\n{full_text}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')\n",
    "\n",
    "\n",
    "# EVALUATE\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=[original_model_text_output],\n",
    "    references=[baseline_human_summary],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=[peft_model_text_output],\n",
    "    references=[baseline_human_summary],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
