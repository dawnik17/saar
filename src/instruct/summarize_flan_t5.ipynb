{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (23.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-08-19 18:49:53.071751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-19 18:49:54.490865: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-19 18:49:54.490982: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-19 18:49:54.490997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, \n",
    "                          AutoTokenizer, \n",
    "                          GenerationConfig, \n",
    "                          TrainingArguments, \n",
    "                          Trainer)\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load peft model\n",
    "# from peft import PeftModel, PeftConfig\n",
    "\n",
    "# peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "#                                        './checkpoint/', \n",
    "#                                        torch_dtype=torch.bfloat16,\n",
    "#                                        is_trainable=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load original model\n",
    "# model_name='google/flan-t5-base'\n",
    "# # model_name = \"./checkpoint/\"\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeftModel:\n",
    "    @staticmethod\n",
    "    def load_base_model(model_path=\"google/flan-t5-base\"):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.bfloat16\n",
    "        ).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_peft_adapter(\n",
    "        base_model_path, peft_model_path, train=False\n",
    "    ):\n",
    "        model, tokenizer = self.load_base_model(base_model_path)\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model, peft_model_path, torch_dtype=torch.bfloat16, is_trainable=False\n",
    "        ).to(device)\n",
    "\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "        if train:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # merge the adapter to the main model\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def save_peft_adapter(model, tokenizer, model_path):\n",
    "        model.save_pretrained(model_path)\n",
    "        tokenizer.save_pretrained(model_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_peft_and_save(model, tokenizer, model_path):\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(model_path)\n",
    "        tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original model\n",
    "name='google/flan-t5-base'\n",
    "model, tokenizer = PeftModel.load_base_model(model_path=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 - Load Dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and aggregate raw data\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Specify the folder path containing the JSON files\n",
    "folder_path = './data'\n",
    "\n",
    "# Initialize an empty list to aggregate the data\n",
    "data = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read and parse JSON data from the file\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            file_data = json.load(json_file)\n",
    "            \n",
    "            # Assuming each JSON file contains a list of dictionaries\n",
    "            if isinstance(file_data, list):\n",
    "                data.extend(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# curate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44131"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "data = [news for news in data if news[\"full_text\"] != \"\" and \"JavaScript is not available\" not in news[\"full_text\"]]\n",
    "random.shuffle(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"Supreme Court rejects Google's plea against record ₹1,337 crore fine\",\n",
       " 'summary': \"The Supreme Court on Thursday refused to entertain Google's plea against the record ₹1,337 crore in an antitrust case for abusing its dominant position in Android. The apex court, however, gave Google a week's extension to deposit 10% of the penalty. Earlier, the Competition Commission of India asked Google to change restrictions imposed on smartphone makers related to pre-installing apps.\",\n",
       " 'link': 'https://www.hindustantimes.com/technology/supreme-court-rejects-google-s-plea-against-anti-trust-body-s-rs-1-337-cr-fine-101674128214718-amp.html?utm_campaign=fullarticle&utm_medium=referral&utm_source=inshorts',\n",
       " 'image_link': 'https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2023/01_jan/19_thu/img_1674134417519_29.jpg?',\n",
       " 'source': 'inshorts',\n",
       " 'full_text': \"The Supreme Court on Thursday refused to entertain the plea of US tech giant Google against an order of the National Company Law Appellate Tribunal (NCLAT) refusing an interim stay on the competition regulator imposing a ₹1,337 crore penalty on it. The top court asked the NCLAT to decide Google's appeal against the competition regulator's order by March 31 this year. (File) {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}} A bench of Chief Justice DY Chandrachud and Justices PS Narasimha and J B Pardiwala granted the US firm seven days to deposit 10 per cent of the penalty imposed by the Competition Commission of India (CCI). The top court asked the NCLAT to decide Google's appeal against the competition regulator's order by March 31 this year. The US firm has been asked to approach the NCLAT within three working days from Thursday for seeking adjudication on its appeal against the CCI order. The NCLAT had on January 4 refused to grant an interim stay on the order of the competition regulator and asked Google to deposit 10 per cent of the penalty amount. The NCLAT had admitted the search giant's challenge to the CCI slapping a ₹1,337.76 crore fine for abusing the dominant position of its Android smartphone operating system in the country. {{^userSubscribed}} {{/userSubscribed}} {{^userSubscribed}} {{/userSubscribed}}\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for news in data:   \n",
    "            input_prompt, label = self._get_summary_prompt(news)\n",
    "            self.inputs.append(input_prompt)\n",
    "            self.labels.append(label)\n",
    "            \n",
    "            input_prompt, label = self._get_title_prompt(news)\n",
    "            self.inputs.append(input_prompt)\n",
    "            self.labels.append(label)\n",
    "            \n",
    "        \"\"\"\n",
    "        Combine the lists using zip\n",
    "        Shuffle the combined list\n",
    "        Unpack the shuffled pairs back into separate lists\n",
    "        And then tokenize\n",
    "        \"\"\"\n",
    "        combined = list(zip(self.inputs, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.inputs, self.labels = zip(*combined)\n",
    "\n",
    "        # tokenize\n",
    "        self.inputs = tokenizer(self.inputs, \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\").input_ids\n",
    "\n",
    "        self.labels = tokenizer(self.labels, \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\").input_ids\n",
    "            \n",
    "    def __len__(self): \n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_summary_prompt(example):\n",
    "        # word count round off\n",
    "        multiple = 25\n",
    "        word_count = len(example[\"summary\"].split())\n",
    "        word_count = int(round(word_count / multiple)) * multiple\n",
    "        \n",
    "        start_prompt = f'Summarize this news article in {word_count} words.\\n\\n'\n",
    "        end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "        prompt = start_prompt + example[\"full_text\"] + end_prompt\n",
    "\n",
    "        return prompt, example[\"summary\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_title_prompt(example):\n",
    "        # word count round off\n",
    "        multiple = 5\n",
    "        word_count = len(example[\"title\"].split())\n",
    "        word_count = int(ceil(word_count / multiple)) * multiple\n",
    "        \n",
    "        start_prompt = f'Give a title to the given news article in not more than {word_count} words.\\n\\n'\n",
    "        mid_prompt = '\\n\\nSummary: '\n",
    "        end_prompt = '\\n\\nTitle: '\n",
    "\n",
    "        prompt = start_prompt + example[\"full_text\"] + mid_prompt + example[\"summary\"] + end_prompt\n",
    "        return prompt, example[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TextDataset(data, tokenizer)\n",
    "# test_data = TextDataset(data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([12198,  1635,  1737,    48,  1506,  1108,    16,   943,  1234,     5,\n",
       "          8642, 13877,    29, 18602,    15,    17,    18, 10477,  2504,  1859,\n",
       "          2618, 21269,  2091,     9,     3, 24266,  6697, 26205,    52,  2659,\n",
       "          1047,    71, 19271,    21,   112,     3, 25434,   161, 29136,    11,\n",
       "         29395,     3, 27759,    44,     8,  3134,     7,     5,    37,   146,\n",
       "            32,    33, 24558,    21,     8,   638, 17551,    32,  5500, 10109,\n",
       "             7,    16,     8, 13877,  6552,  3815,    41,   434,  5329,   201,\n",
       "            28, 21269,  2091,     9, 14268,    53,     8,   596,     5,  2659,\n",
       "          1047,    19,  1083,     8,   163, 15839,    16,     8,   420,   662,\n",
       "            30,     8,     3, 24291, 16890,    16,    66,   386, 10874,     5,\n",
       "           216,    92,  1310,  2237,  6697,    12,     3,     9,     3, 19423,\n",
       "          2300,   939,   872, 14710,    16,  8642, 13877,     3,  3565,     3,\n",
       "             9,     3, 28595,    15,   939,    28,     8,  3795,     5,    86,\n",
       "             8,  4912,   301,  5329,   774,     6,     8,  2059,    18,  1201,\n",
       "            18,  1490,    19,  1107,   326,     3,     9, 17593,     3, 15442,\n",
       "           326,     3,  3390, 23113,    12,   199,     8,  5500, 10109,     7,\n",
       "            12, 15389,   323,     3, 25312,   581, 10987,    15, 14240,    23,\n",
       "          6230,    28,     3,     9,  1996,    12,  8179,     5,    94,    47,\n",
       "            92,  2659,  1047,    31,     7,     3,   324,   189,   332,  1755,\n",
       "          2646,     6,   511,   163,    12,     3,    31,  7846,    15,  7930,\n",
       "            31,  4409, 20338,   109,  4743, 15070, 14734,    12,  5324,    13,\n",
       "          1547,     6, 21269,  2091,     9,     3,   107, 10990,     8,  6697,\n",
       "         27375,    31,     7,   761, 10030,    11,   161, 29136,     5,    96,\n",
       "           279,     9,  1047,   744,    31,    17,  2036,   112, 17484,     5,\n",
       "           148,    56,   373,   217,   376,    28,   112, 17484,    30,     5,\n",
       "           486,     8,  3134,     7,     6,     3,    88,   373,    65,   112,\n",
       "         17484,    30,    11,  3795,     7,  1200,    11,  1200,     5,   216,\n",
       "            19,   373,     3, 27759,    11, 21656,    16,   614,     5,   216,\n",
       "          2746,    12,  1172,    11,  1604,  4604,    12,    36,     3,     9,\n",
       "           394,  1959,   976,     3,    88,   243,     5, 21269,  2091,     9,\n",
       "            92, 15240,  2271,  1547,    81,  2659,  1047,   271,   168,    18,\n",
       "         28822,    12,  2213,    16,     8,     3,  4685,  3826,  3802,    11,\n",
       "          1150,  3802,   581,   135,     5,    96,   279,     9,  1047,    19,\n",
       "           373,  2657,     5,   216,    19,   168,  2657,    21,   321,     8,\n",
       "          3826,  3802,    11,  1150,  3802,    41,   208,     7,  1547,   137,\n",
       "           216,    19,     3,     9, 15839,   113,    54, 19314,   136,  1910,\n",
       "             5,   493,    34,     3,     9,  1131,  1996,    42,   872,  1996,\n",
       "             6,  2300,  1910,    42,     8,     3, 31115,  1910,    41,   382,\n",
       "          1755,   196,     7,   201,  2659,  1047,    19,   373,  2657,    21,\n",
       "           334,  1921,    11,  1706,   976,     3,    88,   974,     5,  6697,\n",
       "           522,   326,   581,    70, 11508,    18, 25295,     7,  1547,    16,\n",
       "            70,   511,  1531,    71,  1588,    13,     8,  3826,  3802,    44,\n",
       "          4826,   109,  5768,    15,    30,  1600,  1682,    37,  2323,    56,\n",
       "           952,   541,   942,    16,     8,  1355,   662,     7,  1726,   225,\n",
       "           321,  2323,  9448,    11,  3673,    16,     8,  1907, 13604,     5,\n",
       "            37,  6578,  6079,     7,    33,    92,  2633,    26,    12, 21500,\n",
       "            16,     8,  1150,  3802,    24,  6963,     8,  3826,  3802,    44,\n",
       "             8, 13346, 12524,  5073,    23, 12750,    16, 22928,     9,  5514,\n",
       "            30,  1797,   968,    41,  5522,   483,    45,   627,   189,    12,\n",
       "           968,   189,   341,  1426,    12,     3, 24291,  9973,   137,    96,\n",
       "          3845,    19,    16,     8,   420,   220,    16,    66, 10874,   121,\n",
       "             3,     1]),\n",
       " tensor([ 7772,    53,    81,  2659,  1047,    71, 19271,     6,   113,  1310,\n",
       "          1632,     8,   511, 18096,    49,    12, 25669,   335,   332,  1755,\n",
       "          3986,   826,   112,     3, 15442,    16, 13877,  6552,  3815,    41,\n",
       "           434,  5329,    61,   460,  2773,  1588,     6,  8642, 13877,    29,\n",
       "         18096,    49,  2504,  1859,  2618, 21269,  2091,     9,   243,     6,\n",
       "            96,   279,     9,  1047,   744,    31,    17,  2036,   112, 17484,\n",
       "           535,    96,  3845,  3795,     7,  1200,    11,  1200,   976,   974,\n",
       "         21269,  2091,     9,     5,  2659,  1047,  5799,     3, 15442,   298,\n",
       "          9085,   638, 17551,    32,  5500, 10109,     7,    16,    46,   301,\n",
       "          5329,   460,  2773,  1588,   581, 10987,    15, 13622,     7,     5,\n",
       "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    \n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    \n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FULL MODEL TRAINING\n",
    "# EPOCH = 1\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#                                   save_steps=5000,\n",
    "#                                   warmup_steps=10,\n",
    "#                                   logging_steps=100,\n",
    "#                                   weight_decay=0.01,\n",
    "#                                   num_train_epochs=EPOCH,\n",
    "#                                   logging_dir='./logs',\n",
    "#                                   output_dir='./checkpoint',\n",
    "#                                   per_device_eval_batch_size=32,\n",
    "#                                   per_device_train_batch_size=32)\n",
    "\n",
    "# Trainer(model=model,\n",
    "#         args=training_args,\n",
    "#         eval_dataset=test_data,\n",
    "#         train_dataset=train_data,\n",
    "#         data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]), \n",
    "#                                     'labels': torch.stack([f[1] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 7077888\n",
      "all model parameters: 254655744\n",
      "percentage of trainable model parameters: 2.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13783' max='55165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13783/55165 1:19:36 < 3:59:02, 2.89 it/s, Epoch 1.25/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.199000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PEFT MODEL TRAINING\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "EPOCH = 5\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))\n",
    "\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "                                  # save_steps=5000,\n",
    "                                  save_strategy=\"no\",\n",
    "                                  warmup_steps=10,\n",
    "                                  logging_steps=1000,\n",
    "                                  weight_decay=0.01,\n",
    "                                  num_train_epochs=EPOCH,\n",
    "                                  logging_dir='./logs',\n",
    "                                  output_dir='./checkpoint',\n",
    "                                  learning_rate=0.0001,\n",
    "                                  auto_find_batch_size=True)\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "                model=peft_model,\n",
    "                args=peft_training_args,\n",
    "                train_dataset=train_data,\n",
    "                data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]), \n",
    "                                            'labels': torch.stack([f[1] for f in data])})\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge peft with main model\n",
    "# and save the model\n",
    "model_path = \"./checkpoint/\"\n",
    "PeftModel.merge_peft_and_save(model=peft_model, tokenizer=tokenizer, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pretrained flan t5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original model\n",
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "An old video showing the Trinity Test, the first atomic bomb test, in US in 1945 has resurfaced on social media. Several people compared the footage with the scene in Christopher Nolan's film 'Oppenheimer'. \"Nolan portrayed the explosion extremely accurate,\" a social media user said. \"Absolutely terrifying,\" said another user.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "A video claiming to be the actual footage of the Trinity Test has surfaced on Instagram.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: A video claiming to be the actual footage of the Trinity Test has surfaced on Instagram. The video shows J Robert Oppenheimer and his team prepping for the historic explosion test. Men at the site felt the heat of a desert sun. Then came the shockwave. Commenters said, \"Both absolutely beautiful and terrifying at the same time.\"\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "index = 5678\n",
    "news = data[index]\n",
    "full_text = news['full_text']\n",
    "baseline_human_summary = news['summary']\n",
    "\n",
    "# word count round off\n",
    "multiple = 25\n",
    "word_count = len(baseline_human_summary.split())\n",
    "word_count = int(round(word_count / multiple)) * multiple\n",
    "\n",
    "start_prompt = f'Summarize this news article in {word_count} words.\\n\\n'\n",
    "end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "prompt = start_prompt + full_text + end_prompt\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = original_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'FULL TEXT:\\n{full_text}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')\n",
    "\n",
    "\n",
    "# EVALUATE\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=[original_model_text_output],\n",
    "    references=[baseline_human_summary],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=[peft_model_text_output],\n",
    "    references=[baseline_human_summary],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
