{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53b3463-483d-4b00-9586-fac4f6a254c8",
   "metadata": {},
   "source": [
    "# **INSHORTS** (https://inshorts.me/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6383c9f-0d1c-4077-bb99-034d31677458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_data(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to fetch data. Status code:\", response.status_code)\n",
    "        data = dict()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data_with_count_trial(url):\n",
    "    counts = [100, 500] + [(i+1) * 1000 for i in range(10)]    \n",
    "    last_result = None\n",
    "    \n",
    "    for idx, count in tqdm(enumerate(counts)):\n",
    "        transit_url = url.format(count=count)\n",
    "        response = requests.get(transit_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            last_result = response.json()\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return last_result[\"data\"][\"articles\"]\n",
    "\n",
    "print_ = lambda x: print(x, \"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df1531d-bb7e-4d49-87fb-bd6a77256af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_url = \"https://inshorts.me/news/all?offset=0&limit={count}\"\n",
    "top_news_url = \"https://inshorts.me/news/top?offset=0&limit={count}\"\n",
    "trending_news_url = \"https://inshorts.me/news/trending?offset=0&limit={count}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e367e841-7641-4794-b2b1-d99680d25ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 unique dates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'09-09-23'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timestamp_to_date(timestamp_ms):\n",
    "    timestamp_seconds = timestamp_ms / 1000  # Convert milliseconds to seconds\n",
    "\n",
    "    # Convert timestamp to datetime object\n",
    "    date_object = datetime.fromtimestamp(timestamp_seconds)\n",
    "\n",
    "    # Format the datetime object as mm-dd-yy\n",
    "    formatted_date = date_object.strftime('%m-%d-%y')\n",
    "\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "dates = set()\n",
    "for url in [all_news_url, top_news_url, trending_news_url]:\n",
    "    timestamp = get_data(url.format(count=1))[\"data\"][\"articles\"][0][\"createdAt\"]\n",
    "    date = timestamp_to_date(timestamp)\n",
    "    dates.add(date)\n",
    "\n",
    "    \n",
    "print(f\"{len(dates)} unique dates\")\n",
    "date = dates.pop()\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467b6e6-08f8-4abc-8066-acd3d7da4ed6",
   "metadata": {},
   "source": [
    "# all news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714ef95b-3464-4d2a-ad5f-f52880346bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:16,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_news = get_data_with_count_trial(all_news_url)\n",
    "print_(len(all_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72c609-e516-4f33-9347-819130d3539a",
   "metadata": {},
   "source": [
    "# top news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1d4aa0-d6fe-4c6e-9133-059fe425a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:19,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3954 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_news = get_data_with_count_trial(top_news_url)\n",
    "print_(len(top_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fec77-80e5-4ed5-959c-55dd7b9f1514",
   "metadata": {},
   "source": [
    "# trending news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abab4a67-b944-4a0b-be10-8ce351534e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:07,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trending_news = get_data_with_count_trial(trending_news_url)\n",
    "print_(len(trending_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfaa429-d0de-4e9b-b732-df1f3422f55c",
   "metadata": {},
   "source": [
    "# news by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0afc57-18fc-48e9-af98-a6bf1b0b80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 18/18 [00:09<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# the below 2 are to be used together\n",
    "get_all_topics = get_data(\"https://inshorts.me/news/topics\")\n",
    "topic_news_api = lambda topic: get_data(f\"https://inshorts.me/news/topics/{topic}\")[\"data\"][\"articles\"]\n",
    "\n",
    "# get topic names\n",
    "all_topics = [topic[\"topic\"] for topic in get_all_topics[\"data\"][\"topics\"]]\n",
    "\n",
    "# topic news\n",
    "topic_news = []\n",
    "for topic in tqdm(all_topics):\n",
    "    topic_news.extend(topic_news_api(topic))\n",
    "    \n",
    "print_(len(topic_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc80ed-7895-4237-bb38-d183be02fbe3",
   "metadata": {},
   "source": [
    "# news by query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dba903-8202-4a26-a4e2-8fca74ea9c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/18 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.60s/it]\u001b[A\n",
      "2it [00:17,  8.50s/it]\u001b[A\n",
      "  6%|███▌                                                            | 1/18 [00:17<04:49, 17.01s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.45s/it]\u001b[A\n",
      "2it [00:06,  3.49s/it]\u001b[A\n",
      "3it [00:16,  5.48s/it]\u001b[A\n",
      " 11%|███████                                                         | 2/18 [00:33<04:26, 16.69s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  2.70it/s]\u001b[A\n",
      "2it [00:17,  8.70s/it]\u001b[A\n",
      " 17%|██████████▋                                                     | 3/18 [00:50<04:15, 17.01s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.52s/it]\u001b[A\n",
      "2it [00:06,  3.68s/it]\u001b[A\n",
      "3it [00:26,  8.68s/it]\u001b[A\n",
      " 22%|██████████████▏                                                 | 4/18 [01:16<04:48, 20.57s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  2.28it/s]\u001b[A\n",
      "2it [00:11,  5.51s/it]\u001b[A\n",
      " 28%|█████████████████▊                                              | 5/18 [01:27<03:42, 17.13s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  3.11it/s]\u001b[A\n",
      "2it [00:05,  3.17s/it]\u001b[A\n",
      "3it [00:24,  8.17s/it]\u001b[A\n",
      " 33%|█████████████████████▎                                          | 6/18 [01:52<03:55, 19.64s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.43it/s]\u001b[A\n",
      "2it [00:11,  5.68s/it]\u001b[A\n",
      " 39%|████████████████████████▉                                       | 7/18 [02:03<03:06, 16.93s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.65s/it]\u001b[A\n",
      "2it [00:12,  6.04s/it]\u001b[A\n",
      " 44%|████████████████████████████▍                                   | 8/18 [02:15<02:33, 15.39s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.60s/it]\u001b[A\n",
      "2it [00:06,  3.44s/it]\u001b[A\n",
      "3it [00:16,  5.57s/it]\u001b[A\n",
      " 50%|████████████████████████████████                                | 9/18 [02:32<02:22, 15.80s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  2.08it/s]\u001b[A\n",
      "2it [00:10,  5.40s/it]\u001b[A\n",
      " 56%|███████████████████████████████████                            | 10/18 [02:43<01:54, 14.26s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.38s/it]\u001b[A\n",
      "2it [00:13,  6.66s/it]\u001b[A\n",
      " 61%|██████████████████████████████████████▌                        | 11/18 [02:56<01:37, 13.97s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.03s/it]\u001b[A\n",
      "2it [00:08,  4.85s/it]\u001b[A\n",
      "3it [00:27,  9.31s/it]\u001b[A\n",
      " 67%|██████████████████████████████████████████                     | 12/18 [03:24<01:49, 18.22s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.74it/s]\u001b[A\n",
      "2it [00:11,  5.57s/it]\u001b[A\n",
      " 72%|█████████████████████████████████████████████▌                 | 13/18 [03:35<01:20, 16.08s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.33s/it]\u001b[A\n",
      "2it [00:11,  5.99s/it]\u001b[A\n",
      " 78%|█████████████████████████████████████████████████              | 14/18 [03:47<00:59, 14.85s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  3.09it/s]\u001b[A\n",
      "2it [00:05,  2.95s/it]\u001b[A\n",
      "3it [00:15,  5.12s/it]\u001b[A\n",
      " 83%|████████████████████████████████████████████████████▌          | 15/18 [04:03<00:45, 15.00s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.73s/it]\u001b[A\n",
      "2it [00:12,  6.05s/it]\u001b[A\n",
      " 89%|████████████████████████████████████████████████████████       | 16/18 [04:15<00:28, 14.13s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  2.74it/s]\u001b[A\n",
      "2it [00:05,  3.28s/it]\u001b[A\n",
      "3it [00:16,  5.35s/it]\u001b[A\n",
      " 94%|███████████████████████████████████████████████████████████▌   | 17/18 [04:31<00:14, 14.71s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  2.36it/s]\u001b[A\n",
      "2it [00:05,  3.10s/it]\u001b[A\n",
      "3it [00:23,  7.82s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████| 18/18 [04:54<00:00, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7580 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search_news_api = lambda query: get_data_with_count_trial(\"https://inshorts.me/news/search?query={query}&offset=0&limit={{count}}\".format(query=query))\n",
    "\n",
    "# topic news\n",
    "search_news = []\n",
    "\n",
    "for topic in tqdm(all_topics):\n",
    "    search_news.extend(search_news_api(query=topic))\n",
    "    \n",
    "print_(len(search_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec4430b-bf03-479f-877c-1ae02002ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14892 news\n"
     ]
    }
   ],
   "source": [
    "clubbed_overall_news = []\n",
    "\n",
    "\n",
    "for news in all_news + top_news + trending_news + topic_news + search_news:\n",
    "    clubbed_overall_news.append(\n",
    "        {\"title\": news[\"title\"].strip(),\n",
    "         \"summary\": news[\"content\"].strip(),\n",
    "         \"link\": news[\"sourceUrl\"],\n",
    "         \"image_link\": news[\"imageUrl\"],\n",
    "         \"source\": \"inshorts\"}\n",
    "    )\n",
    "    \n",
    "print_(len(clubbed_overall_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b99e7d-449e-419f-8e9d-ed3cf0340e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"How is India-Middle East-Europe Corridor different from China's Belt & Road Initiative?\",\n",
       " 'summary': 'The India-Middle East-Europe Economic Corridor will be substantially different from China\\'s Belt and Road Initiative, Railways Minister Ashwini Vaishnaw said. Unlike the BRI, where a huge debt burden gets imposed on host nations, the G20 project will bring in revenue and be bankable, Vaishnaw added. \"The BRI came with...conditions...[Now] countries can decide on basis of its needs,\" he added.',\n",
       " 'link': 'https://www.news18.com/amp/videos/india/railway-minister-on-how-india-middle-east-eu-corridor-differs-from-china-s-bri-g20-summit-news18-8572165.html?utm_campaign=fullarticle&utm_medium=referral&utm_source=inshorts',\n",
       " 'image_link': 'https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2023/09_sep/10_sun/img_1694367018282_376.jpg?',\n",
       " 'source': 'inshorts'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clubbed_overall_news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51e21e-0ba3-4c4c-ab3e-d1441d63e7dd",
   "metadata": {},
   "source": [
    "# Deduplicate News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851a3ec5-ea31-4196-91ff-6cbdd38d4af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9832  unique links\n",
      "14892  total links\n"
     ]
    }
   ],
   "source": [
    "print(len({n[\"link\"] for n in clubbed_overall_news}), \" unique links\")\n",
    "print(len(clubbed_overall_news), \" total links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e7e36d-6edc-46bd-99cd-5113c2c649e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9944 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dawn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from saar.utils import deduplicate_list_of_dicts, get_full_news\n",
    "\n",
    "keys_to_check = ['title', 'summary', 'link']\n",
    "clubbed_overall_news = deduplicate_list_of_dicts(clubbed_overall_news, keys_to_check)\n",
    "\n",
    "print_(len(clubbed_overall_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff5fb6-ff99-4316-bb6c-610e19f3f06d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **RSS FEEDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dc45cc4-5765-49f2-9d15-a190e5256166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rss_feeds\n",
    "# import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bf83776-23fe-47bf-b7ec-012a12574cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [02:52<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3851 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# feed_urls = list(set(rss_feeds.rss_feeds))\n",
    "# feed_news = []\n",
    "\n",
    "# for feed_url in tqdm(feed_urls):\n",
    "#     feed = feedparser.parse(feed_url)\n",
    "    \n",
    "#     # Iterate through the entries in the feed\n",
    "#     for entry in feed.entries:\n",
    "#         try:\n",
    "#             feed_news.append(\n",
    "#                 {\"title\": entry.title,\n",
    "#                  \"summary\": entry.summary,\n",
    "#                  \"link\": entry.link,\n",
    "#                  \"source\": \"rss feed\"}\n",
    "#             )\n",
    "#         except:\n",
    "#             continue\n",
    "            \n",
    "# print_(len(feed_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f95a71f8-65df-44b6-9707-70f535707a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Families of babies murdered by nurse Lucy Letby vow to continue their search for answers',\n",
       " 'summary': 'The families of babies murdered by Lucy Letby have vowed to continue their search for answers as questions swirled around what more could have been done to stop her killing spree.',\n",
       " 'link': 'https://news.sky.com/story/families-of-babies-murdered-by-lucy-letby-vow-to-continue-their-search-for-answers-12942744',\n",
       " 'source': 'rss feed'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed_news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cb7d5-8cc3-45d3-8bcf-3680e051dba5",
   "metadata": {},
   "source": [
    "# **AGGREGATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6558d656-8f37-4257-b536-20d5c67e5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = clubbed_overall_news #+ feed_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c685d-ee4c-45f9-8371-52ec0b7f2058",
   "metadata": {},
   "source": [
    "# Full Text from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b335d-9d1c-4135-8a60-7cc5101e3655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███                                                       | 519/9944 [43:55<2:58:57,  1.14s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "get full news\n",
    "\"\"\"\n",
    "news = [get_full_news(new) for new in tqdm(news)]\n",
    "news = [new for new in news if new is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4516b1-a640-47bd-9f1a-57545f2a3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../../')\n",
    "from saar.infer import Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "494d8edb-d519-4602-81d6-b0578e25377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "run inference\n",
    "\"\"\"\n",
    "if len(news) > 0:\n",
    "    # summary adapter, title adapter path\n",
    "    summary_adapter_path = os.environ[\"SUMMARY_ADAPTER_PATH\"]\n",
    "    title_adapter_path = os.environ[\"TITLE_ADAPTER_PATH\"]\n",
    "\n",
    "    logging.info(\"loading models..\")\n",
    "    infer = Infer(\n",
    "        summary_adapter_path=summary_adapter_path, title_adapter_path=title_adapter_path\n",
    "    )\n",
    "\n",
    "    # batching\n",
    "    batch = lambda data, batch_size: [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    \n",
    "    batch_size = os.environ[\"BATCH_SIZE\"]\n",
    "    news = batch(news, batch_size=batch_size)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    logging.info(\"running inference..\")\n",
    "    for news_batch in tqdm(news):\n",
    "        # generate summary\n",
    "        news_batch = infer(mode=\"summary\", data=news_batch)\n",
    "\n",
    "        # generate title\n",
    "        # NOTE: Title generation needs summary already generated as \"generated_summary\" key in the news dict\n",
    "        news_batch = infer(mode=\"title\", data=news_batch)\n",
    "        data.extend(news_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d954051-d295-4a79-a546-4d7afa703429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = f\"../../../data/training/{date}.json\"\n",
    "\n",
    "with open(path, 'w') as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
