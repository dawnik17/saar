{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53b3463-483d-4b00-9586-fac4f6a254c8",
   "metadata": {},
   "source": [
    "# **INSHORTS** (https://inshorts.me/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6383c9f-0d1c-4077-bb99-034d31677458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_data(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to fetch data. Status code:\", response.status_code)\n",
    "        data = dict()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data_with_count_trial(url):\n",
    "    counts = [100, 500] + [(i+1) * 1000 for i in range(10)]    \n",
    "    last_result = None\n",
    "    \n",
    "    for idx, count in tqdm(enumerate(counts)):\n",
    "        transit_url = url.format(count=count)\n",
    "        response = requests.get(transit_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            last_result = response.json()\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return last_result[\"data\"][\"articles\"]\n",
    "\n",
    "print_ = lambda x: print(x, \"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df1531d-bb7e-4d49-87fb-bd6a77256af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_url = \"https://inshorts.me/news/all?offset=0&limit={count}\"\n",
    "top_news_url = \"https://inshorts.me/news/top?offset=0&limit={count}\"\n",
    "trending_news_url = \"https://inshorts.me/news/trending?offset=0&limit={count}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e367e841-7641-4794-b2b1-d99680d25ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 unique dates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'08-19-23'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timestamp_to_date(timestamp_ms):\n",
    "    timestamp_seconds = timestamp_ms / 1000  # Convert milliseconds to seconds\n",
    "\n",
    "    # Convert timestamp to datetime object\n",
    "    date_object = datetime.fromtimestamp(timestamp_seconds)\n",
    "\n",
    "    # Format the datetime object as mm-dd-yy\n",
    "    formatted_date = date_object.strftime('%m-%d-%y')\n",
    "\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "dates = set()\n",
    "for url in [all_news_url, top_news_url, trending_news_url]:\n",
    "    timestamp = get_data(url.format(count=1))[\"data\"][\"articles\"][0][\"createdAt\"]\n",
    "    date = timestamp_to_date(timestamp)\n",
    "    dates.add(date)\n",
    "\n",
    "    \n",
    "print(f\"{len(dates)} unique dates\")\n",
    "date = dates.pop()\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467b6e6-08f8-4abc-8066-acd3d7da4ed6",
   "metadata": {},
   "source": [
    "# all news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714ef95b-3464-4d2a-ad5f-f52880346bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_news = get_data_with_count_trial(all_news_url)\n",
    "print_(len(all_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72c609-e516-4f33-9347-819130d3539a",
   "metadata": {},
   "source": [
    "# top news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1d4aa0-d6fe-4c6e-9133-059fe425a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:37,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2981 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_news = get_data_with_count_trial(top_news_url)\n",
    "print_(len(top_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fec77-80e5-4ed5-959c-55dd7b9f1514",
   "metadata": {},
   "source": [
    "# trending news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abab4a67-b944-4a0b-be10-8ce351534e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:43,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trending_news = get_data_with_count_trial(trending_news_url)\n",
    "print_(len(trending_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfaa429-d0de-4e9b-b732-df1f3422f55c",
   "metadata": {},
   "source": [
    "# news by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0afc57-18fc-48e9-af98-a6bf1b0b80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# the below 2 are to be used together\n",
    "get_all_topics = get_data(\"https://inshorts.me/news/topics\")\n",
    "topic_news_api = lambda topic: get_data(f\"https://inshorts.me/news/topics/{topic}\")[\"data\"][\"articles\"]\n",
    "\n",
    "# get topic names\n",
    "all_topics = [topic[\"topic\"] for topic in get_all_topics[\"data\"][\"topics\"]]\n",
    "\n",
    "# topic news\n",
    "topic_news = []\n",
    "for topic in tqdm(all_topics):\n",
    "    topic_news.extend(topic_news_api(topic))\n",
    "    \n",
    "print_(len(topic_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc80ed-7895-4237-bb38-d183be02fbe3",
   "metadata": {},
   "source": [
    "# news by query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dba903-8202-4a26-a4e2-8fca74ea9c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.89s/it]\u001b[A\n",
      "2it [00:18,  9.25s/it]\u001b[A\n",
      "  5%|▌         | 1/20 [00:18<05:51, 18.51s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.09s/it]\u001b[A\n",
      "2it [00:20, 10.44s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:39<05:58, 19.91s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.92s/it]\u001b[A\n",
      "2it [00:07,  3.99s/it]\u001b[A\n",
      "3it [00:26,  8.76s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [01:05<06:27, 22.82s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.16s/it]\u001b[A\n",
      "2it [00:08,  4.62s/it]\u001b[A\n",
      "3it [00:28,  9.54s/it]\u001b[A\n",
      " 20%|██        | 4/20 [01:34<06:41, 25.11s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.56s/it]\u001b[A\n",
      "2it [00:23, 11.62s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [01:57<06:06, 24.44s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.23s/it]\u001b[A\n",
      "2it [00:20, 10.26s/it]\u001b[A\n",
      " 30%|███       | 6/20 [02:18<05:23, 23.11s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.49s/it]\u001b[A\n",
      "2it [00:18,  9.48s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [02:37<04:42, 21.76s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.85s/it]\u001b[A\n",
      "2it [00:19,  9.85s/it]\u001b[A\n",
      " 40%|████      | 8/20 [02:56<04:13, 21.11s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.55s/it]\u001b[A\n",
      "2it [00:19, 10.00s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [03:16<03:48, 20.76s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.08s/it]\u001b[A\n",
      "2it [00:21, 10.51s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [03:37<03:28, 20.85s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.65s/it]\u001b[A\n",
      "2it [00:08,  4.39s/it]\u001b[A\n",
      "3it [00:30, 10.24s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [04:08<03:34, 23.87s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.57s/it]\u001b[A\n",
      "2it [00:19,  9.74s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [04:28<03:00, 22.54s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.10s/it]\u001b[A\n",
      "2it [00:22, 11.27s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [04:50<02:37, 22.55s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.94s/it]\u001b[A\n",
      "2it [00:10,  5.95s/it]\u001b[A\n",
      "3it [00:30, 10.02s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [05:20<02:28, 24.82s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.93s/it]\u001b[A\n",
      "2it [00:22, 11.39s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [05:43<02:01, 24.21s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.19s/it]\u001b[A\n",
      "2it [00:21, 10.55s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [06:04<01:33, 23.27s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.47s/it]\u001b[A\n",
      "2it [00:07,  4.00s/it]\u001b[A\n",
      "3it [00:26,  8.90s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [06:31<01:12, 24.30s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.10s/it]\u001b[A\n",
      "2it [00:21, 10.79s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [06:52<00:46, 23.49s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.65s/it]\u001b[A\n",
      "2it [00:09,  5.02s/it]\u001b[A\n",
      "3it [00:30, 10.12s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [07:23<00:25, 25.55s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.87s/it]\u001b[A\n",
      "2it [00:07,  4.00s/it]\u001b[A\n",
      "3it [00:27,  9.03s/it]\u001b[A\n",
      "100%|██████████| 20/20 [07:50<00:00, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7167 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search_news_api = lambda query: get_data_with_count_trial(\"https://inshorts.me/news/search?query={query}&offset=0&limit={{count}}\".format(query=query))\n",
    "\n",
    "# topic news\n",
    "search_news = []\n",
    "\n",
    "for topic in tqdm(all_topics):\n",
    "    search_news.extend(search_news_api(query=topic))\n",
    "    \n",
    "print_(len(search_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec4430b-bf03-479f-877c-1ae02002ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13542 news\n"
     ]
    }
   ],
   "source": [
    "clubbed_overall_news = []\n",
    "\n",
    "\n",
    "for news in all_news + top_news + trending_news + topic_news + search_news:\n",
    "    clubbed_overall_news.append(\n",
    "        {\"title\": news[\"title\"].strip(),\n",
    "         \"summary\": news[\"content\"].strip(),\n",
    "         \"link\": news[\"sourceUrl\"],\n",
    "         \"image_link\": news[\"imageUrl\"],\n",
    "         \"source\": \"inshorts\"}\n",
    "    )\n",
    "    \n",
    "print_(len(clubbed_overall_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b99e7d-449e-419f-8e9d-ed3cf0340e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"ED seizes ₹36-cr asset of Goa miner's son in Pandora Papers case\",\n",
       " 'summary': \"Enforcement Directorate said it has seized an immovable property worth ₹36.8 crore of Rohan Timblo, who is the son of Goa-based miner Radha Timblo. The seizure has been made as part of ED's investigation into the Pandora Papers leak, which allegedly revealed Rohan held undisclosed foreign exchange outside India. He allegedly contravened the provisions of FEMA for about ₹37 crore.\",\n",
       " 'link': 'https://www.bqprime.com/nation/pandora-papers-leak-ed-seizes-over-rs-36-crore-worth-asset-of-goa-miners-son?utm_campaign=fullarticle&utm_medium=referral&utm_source=inshorts',\n",
       " 'image_link': 'https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2023/08_aug/19_sat/img_1692444427680_806.jpg?',\n",
       " 'source': 'inshorts'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clubbed_overall_news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51e21e-0ba3-4c4c-ab3e-d1441d63e7dd",
   "metadata": {},
   "source": [
    "# Deduplicate News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851a3ec5-ea31-4196-91ff-6cbdd38d4af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8742  unique links\n",
      "13542  total links\n"
     ]
    }
   ],
   "source": [
    "print(len({n[\"link\"] for n in clubbed_overall_news}), \" unique links\")\n",
    "print(len(clubbed_overall_news), \" total links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ded3df-4ee9-4536-860e-386f6e16e938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8827 news\n"
     ]
    }
   ],
   "source": [
    "def deduplicate_list_of_dicts(input_list, keys_to_check):\n",
    "    \"\"\"\n",
    "    input_list - list of dictionaries\n",
    "    keys_to_check: deduplicate only on these keys\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    deduplicated_list = []\n",
    "    \n",
    "    for d in input_list:\n",
    "        dict_subset = {key: d[key] for key in keys_to_check if key in d}\n",
    "        dict_tuple = tuple(dict_subset.items())\n",
    "        \n",
    "        if dict_tuple not in seen:\n",
    "            seen.add(dict_tuple)\n",
    "            deduplicated_list.append(d)\n",
    "    \n",
    "    return deduplicated_list\n",
    "\n",
    "keys_to_check = ['title', 'summary', 'link']\n",
    "\n",
    "clubbed_overall_news = deduplicate_list_of_dicts(clubbed_overall_news, keys_to_check)\n",
    "print_(len(clubbed_overall_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff5fb6-ff99-4316-bb6c-610e19f3f06d",
   "metadata": {},
   "source": [
    "# **RSS FEEDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dc45cc4-5765-49f2-9d15-a190e5256166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rss_feeds\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bf83776-23fe-47bf-b7ec-012a12574cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [02:52<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3851 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "feed_urls = list(set(rss_feeds.rss_feeds))\n",
    "feed_news = []\n",
    "\n",
    "for feed_url in tqdm(feed_urls):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    \n",
    "    # Iterate through the entries in the feed\n",
    "    for entry in feed.entries:\n",
    "        try:\n",
    "            feed_news.append(\n",
    "                {\"title\": entry.title,\n",
    "                 \"summary\": entry.summary,\n",
    "                 \"link\": entry.link,\n",
    "                 \"source\": \"rss feed\"}\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "print_(len(feed_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f95a71f8-65df-44b6-9707-70f535707a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Families of babies murdered by nurse Lucy Letby vow to continue their search for answers',\n",
       " 'summary': 'The families of babies murdered by Lucy Letby have vowed to continue their search for answers as questions swirled around what more could have been done to stop her killing spree.',\n",
       " 'link': 'https://news.sky.com/story/families-of-babies-murdered-by-lucy-letby-vow-to-continue-their-search-for-answers-12942744',\n",
       " 'source': 'rss feed'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cb7d5-8cc3-45d3-8bcf-3680e051dba5",
   "metadata": {},
   "source": [
    "# **AGGREGATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6558d656-8f37-4257-b536-20d5c67e5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS = clubbed_overall_news + feed_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c685d-ee4c-45f9-8371-52ec0b7f2058",
   "metadata": {},
   "source": [
    "# Full Text from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "494d8edb-d519-4602-81d6-b0578e25377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import re\n",
    "\n",
    "\n",
    "phrases_to_remove = [\"Sign In\", \"Want to read more?\", \"Already have an account?\", \"To continue reading\"]\n",
    "\n",
    "def remove_phrases(string, phrases):\n",
    "    pattern = '|'.join(re.escape(phrase) for phrase in phrases)\n",
    "    result = re.split(pattern, string)\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "def curate_article(article):\n",
    "    # Remove \"Advertisement\" sections\n",
    "    curated_article = re.sub(r'Advertisement', '', article)\n",
    "\n",
    "    # Remove extra spaces and new lines\n",
    "    curated_article = re.sub(r'\\n{3,}', '\\n\\n', curated_article)\n",
    "    \n",
    "    # Remove everything after the stop phrases\n",
    "    curated_article = remove_phrases(curated_article, phrases_to_remove)\n",
    "    \n",
    "    # routine curation\n",
    "    curated_article = re.sub(r'\\s+', ' ', curated_article)\n",
    "    curated_article = curated_article.strip()\n",
    "\n",
    "    return curated_article\n",
    "\n",
    "\n",
    "def extract_article_text(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return curate_article(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38f765b9-4c01-4215-99fc-3a8fd76abdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "failed: 2025, total: 12678, percentage: 15.97: 100%|██████████| 12678/12678 [3:43:22<00:00,  1.06s/it]   \n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(NEWS)\n",
    "\n",
    "failed = 0\n",
    "total = 0\n",
    "\n",
    "for news in pbar:\n",
    "    try:\n",
    "        full_text = extract_article_text(news[\"link\"])\n",
    "        \n",
    "    except:\n",
    "        full_text = \"\"\n",
    "    \n",
    "    if full_text == \"\":\n",
    "        failed += 1\n",
    "    total += 1\n",
    "    \n",
    "    pbar.set_description(f\"failed: {failed}, total: {total}, failed percentage: {round(100 * failed / total, 2)}\")\n",
    "    news.update({\"full_text\": full_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d954051-d295-4a79-a546-4d7afa703429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = f'./data/{date}.json'\n",
    "\n",
    "with open(path, 'w') as json_file:\n",
    "    json.dump(NEWS, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
