{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-09-05 19:05:32.259120: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-05 19:05:33.735484: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-05 19:05:33.735611: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-05 19:05:33.735627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, \n",
    "                          AutoTokenizer, \n",
    "                          GenerationConfig, \n",
    "                          TrainingArguments, \n",
    "                          Trainer)\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# transformers.__version__ == '4.28.1'\n",
    "\n",
    "# import peft\n",
    "# peft.__version__ == '0.3.0'\n",
    "\n",
    "# import torch\n",
    "# torch.__version__ == \"2.0.0+cu117\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "class PeftModelUtils:\n",
    "    @staticmethod\n",
    "    def load_base_model(model_path=\"google/flan-t5-base\"):\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_peft_adapter(\n",
    "        base_model_path, peft_model_path, train=False, merge_adapter=True\n",
    "    ):\n",
    "        model, tokenizer = PeftModelUtils.load_base_model(base_model_path)\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model, \n",
    "            peft_model_path, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            is_trainable=train,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        if merge_adapter:\n",
    "            model = model.merge_and_unload()\n",
    "\n",
    "            if train:\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # merge the adapter to the main model\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def save_peft_adapter(model, model_path):\n",
    "        model.save_pretrained(model_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_peft_and_save(model, model_path):\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(model_path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def save_tokenizer(tokenizer):\n",
    "        tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load pretrained flan t5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load original model\n",
    "# model_name='google/flan-t5-base'\n",
    "# original_model, original_tokenizer = PeftModelUtils.load_base_model(model_path=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load summary adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "adapter_path = \"./checkpoint/adapter/\"\n",
    "peft_model, tokenizer = PeftModelUtils.load_from_peft_adapter(model_name, adapter_path, merge_adapter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load title adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "adapter_path = \"./checkpoint/title_adapter/\"\n",
    "title_model, tokenizer = PeftModelUtils.load_from_peft_adapter(model_name, adapter_path, merge_adapter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qblocks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# bring live news\n",
    "from newspaper import Article\n",
    "import re\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "phrases_to_remove = [\"Sign In\", \"Want to read more?\", \"Already have an account?\", \"To continue reading\"]\n",
    "\n",
    "def remove_phrases(string, phrases):\n",
    "    pattern = '|'.join(re.escape(phrase) for phrase in phrases)\n",
    "    result = re.split(pattern, string)\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "def curate_article(article):\n",
    "    # Remove \"Advertisement\" sections\n",
    "    curated_article = re.sub(r'Advertisement', '', article)\n",
    "\n",
    "    # Remove extra spaces and new lines\n",
    "    curated_article = re.sub(r'\\n{3,}', '\\n\\n', curated_article)\n",
    "    \n",
    "    # Remove everything after the stop phrases\n",
    "    curated_article = remove_phrases(curated_article, phrases_to_remove)\n",
    "    \n",
    "    # routine curation\n",
    "    curated_article = re.sub(r'\\s+', ' ', curated_article)\n",
    "    curated_article = curated_article.strip()\n",
    "\n",
    "    return curated_article\n",
    "\n",
    "\n",
    "def get_news(news):\n",
    "    url = news[\"link\"]\n",
    "    \n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    \n",
    "    news[\"full_text\"] = curate_article(article.text)\n",
    "    \n",
    "    if \"image_url\" not in news:\n",
    "        news[\"image_url\"] = article.top_image\n",
    "        \n",
    "    if \"date\" not in news:\n",
    "        news[\"date\"] = datetime.strptime(str(article.meta_data[\"pdate\"]), \"%Y%m%d\")\n",
    "    \n",
    "    if \"datetime\" not in news:\n",
    "        news[\"datetime\"] = datetime.strptime(str(article.meta_data[\"pdate\"]), \"%Y%m%d\")\n",
    "        \n",
    "    if \"title\" not in news:\n",
    "        news[\"title\"] = article.title\n",
    "        \n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_generated_text(text):\n",
    "    # Remove extra whitespace and newline characters\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Remove repeated sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text)\n",
    "    \n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "            \n",
    "    cleaned_text = ' '.join(unique_sentences)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetTokens:\n",
    "    def __init__(self, tokenizer):\n",
    "        # device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # summary meta data\n",
    "        summary_word_count = 60\n",
    "\n",
    "        self.summary_start_prompt = f'Summarize this news article in {summary_word_count} words.\\n\\n'\n",
    "        self.summary_end_prompt = '\\n\\nSummary: '\n",
    "        \n",
    "        # title meta data\n",
    "        title_word_count = 5\n",
    "        \n",
    "        self.title_start_prompt = f'Give a title to the given news article in not more than {title_word_count} words.\\n\\n'\n",
    "        self.title_mid_prompt = '\\n\\nSummary: '\n",
    "        self.title_end_prompt = '\\n\\nTitle: '\n",
    "\n",
    "    def get_title_prompt(self, news):\n",
    "        return self.title_start_prompt + news[\"full_text\"] + self.title_mid_prompt + news[\"summary\"] + self.title_end_prompt\n",
    "    \n",
    "    def get_summary_prompt(self, news):\n",
    "        return self.summary_start_prompt + news[\"full_text\"] + self.summary_end_prompt\n",
    "    \n",
    "    def tokenize(self, news: list, mode=\"summary\"):\n",
    "        prompt_func = getattr(self, f'get_{mode}_prompt')\n",
    "        prompts = [prompt_func(news=new) for new in news]\n",
    "        \n",
    "        return self.tokenizer(prompts, \n",
    "                              return_tensors=\"pt\", \n",
    "                              max_length=tokenizer.model_max_length,\n",
    "                              truncation=True,\n",
    "                              padding=True).input_ids.to(self.device)\n",
    "    \n",
    "get_tokens = GetTokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = [{\"link\": \"https://www.ndtv.com/india-news/if-we-name-alliance-bharat-will-they-call-country-bjp-arvind-kejriwal-4361334\"}]#,\n",
    "        # {\"link\": \"https://www.bbc.com/news/world-europe-66712477\"},\n",
    "        # {\"link\": \"https://www.the-independent.com/news/world/americas/us-politics/trump-mugshot-indictment-latest-news-b2404959.html\"}]\n",
    "\n",
    "news = [get_news(new) for new in news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, generation_config, data, mode):\n",
    "    \"\"\"\n",
    "    for summary: data = [{\"full_text\": \"...\"}]\n",
    "    for title: data = [{\"full_text\": \"...\", \"summary\": \"...\"}]\n",
    "    \"\"\"\n",
    "    input_ids = get_tokens.tokenize(news=data, mode=mode)\n",
    "\n",
    "    # peft flan T5\n",
    "    peft_model_outputs = model.generate(input_ids=input_ids, \n",
    "                                        generation_config=generation_config)\n",
    "\n",
    "\n",
    "    for i in range(len(peft_model_outputs)):\n",
    "        data[i][mode] = clean_generated_text(tokenizer.decode(peft_model_outputs[i], skip_special_tokens=True))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary generation config\n",
    "summary_generation_config = GenerationConfig(max_new_tokens=200, \n",
    "                                     num_beams=8,\n",
    "                                     do_sample=False,\n",
    "                                     temperature=1.5,\n",
    "                                     top_k=30,\n",
    "                                     top_p=0.8)\n",
    "\n",
    "news = infer(model=peft_model, \n",
    "             generation_config=summary_generation_config, \n",
    "             mode=\"summary\", \n",
    "             data=news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title generation config\n",
    "title_generation_config = GenerationConfig(max_new_tokens=50, \n",
    "                                         num_beams=8,\n",
    "                                         do_sample=False,\n",
    "                                         temperature=1.5,\n",
    "                                         top_k=30,\n",
    "                                         top_p=0.8)\n",
    "\n",
    "news = infer(model=title_model, \n",
    "             generation_config=title_generation_config, \n",
    "             mode=\"title\", \n",
    "             data=news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'link': 'https://www.ndtv.com/india-news/if-we-name-alliance-bharat-will-they-call-country-bjp-arvind-kejriwal-4361334',\n",
       "  'full_text': 'The Congress has accused the BJP government of \"distorting history and dividing India\". The use of \\'President of Bharat\\' in place of the traditional \\'President of India\\' in an official invite to foreign leaders attending the G20 summit has sparked a flurry of political reactions. While opposition leaders have slammed the move and linked it to their 28-party alliance naming itself INDIA, the BJP has questioned why some parties \"object to every issue related to the honour and pride of the country\". One of the sharpest reactions to the wording used in the invite came from AAP chief Arvind Kejriwal who asked whether the ruling party would change the country\\'s name to \\'BJP\\' if the opposition alliance decided to call itself \\'Bharat\\'. Addressing a press conference on Tuesday, the Delhi chief minister said in Hindi, \"I have no official information that this (a name change) is happening. Just because many opposition parties have formed an alliance and called it INDIA, will the Centre change the name of the country? The country belongs to 140 crore people, not to one party. If the name of the alliance is changed to Bharat, will they change the name of Bharat to BJP?\" Mr Kejriwal\\'s party colleague and Rajya Sabha MP Raghav Chadha also hit out at the BJP and said that the country does not belong to one political party. \"The BJP\\'s recent move to change the reference from \\'President of India\\' to \\'President of Bharat\\' on official G20 summit invitations has raised eyebrows and ignited a public debate. How can the BJP strike down \\'INDIA\\'? The country doesn\\'t belong to a political party; it belongs to 135 crore Indians. Our national identity is not the BJP\\'s personal property that it can modify on whims and fancies.Judega Bharat. Jeetega INDIA,\" Mr Chadha posted on X, formerly Twitter. The BJP\\'s recent move to change the reference from \\'President of India\\' to \\'President of Bharat\\' on official G20 summit invitations has raised eyebrows and ignited a public debate. How can the BJP strike down \\'INDIA\\'? The country doesn\\'t belong to a political party; it belongs to… pic.twitter.com/riYNdQBkYa — Raghav Chadha (@raghav_chadha) September 5, 2023 West Bengal Chief Minister Mamata Banerjee said the whole world knows the country as India and wondered whether the name of Rabindranath Tagore would be changed next. \"I have heard that they are changing the name of India. The invites in the name of the President say Bharat. What\\'s new in this? We say India in English and Bharat in Hindi. Even we say Bharat. But the world knows the country as India. What happened suddenly that the country\\'s name will be changed? Will the name of kabi thakur (Rabindranath Tagore) also be changed,\" she asked. Tamil Nadu Chief Minister and Dravida Munnetra Kazhagam President MK Stalin said the BJP promised to transform India, but all that the country got was a name change after nine years. After Non-BJP forces united to dethrone the fascist BJP regime and aptly named their alliance #INDIA, now the BJP wants to change \\'India\\' for \\'Bharat.\\' BJP promised to TRANSFORM India, but all we got is a name change after 9 years! Seems like the BJP is rattled by a single term… — M.K.Stalin (@mkstalin) September 5, 2023 \"After Non-BJP forces united to dethrone the fascist BJP regime and aptly named their alliance #INDIA, now the BJP wants to change \\'India\\' for \\'Bharat.\\' BJP promised to TRANSFORM India, but all we got is a name change after 9 years! Seems like the BJP is rattled by a single term called India because they recognise the strength of unity within the opposition. During the elections, \\'India\\' will chase BJP out of power,\" the DMK chief posted on X. Addressing a press conference in Maharashtra\\'s Jalgaon district, Nationalist Congress Party chief Sharad Pawar said no one has the right to change the country\\'s name. \"I don\\'t understand why the ruling party is perturbed over a name related to the country,\" the NCP chief said, adding that Congress President Mallikarjun Kharge has called a meeting of INDIA alliance party heads on Wednesday. \"There will be deliberation on this in the meeting, but no one has the right to change the name (of the country). No one can change the name,\" news agency PTI quoted Mr Pawar as saying. Claiming that Prime Minister Narendra Modi is scared of the INDIA alliance, Bihar Deputy Chief Minister and RJD leader Tejashwi Yadav said that India is everywhere from the country\\'s Constitution to its passport. \"PM Modi is scared of the INDIA alliance. We have the President of India, Prime Minister of India. The name is on the passport and Aadhaar cards. The Constitution says \\'We, the people of India\\' and \\'Bharat\\' in Hindi. If he has a problem with the INDIA name, he should have a problem with Bharat as well. Because our slogan is \\'Judega Bharat, Jeetega INDIA,\" Mr Yadav said. \"Till a few days ago, they kept saying \\'Vote for India\\'. Now they want to hide the India name and write Bharat. But there is no difference between India and Bharat. One is in English and one is in Hindi. So these decisions are being taken because they are nervous. How many places will you remove the name from? How much will you spend? Getting the name changed may cost as much as a state\\'s budget. What will happen to Make in India and Skill India,\" he asked. The move also invited scathing criticism from the Congress, which accused the Narendra Modi government of \"distorting history and dividing India\". In a post on X, Congress General Secretary and former Union Minister Jairam Ramesh said, \"So the news is indeed true. Rashtrapati Bhawan has sent out an invite for a G20 dinner on Sept 9th in the name of \\'President of Bharat\\' instead of the usual \\'President of India\\'. Now, Article 1 in the Constitution can read: \\'Bharat, that was India, shall be a Union of States.\\' But now even this \"Union of States\" is under assault.\" Mr. Modi can continue to distort history and divide India, that is Bharat, that is a Union of States. But we will not be deterred. After all, what is the objective of INDIA parties? It is BHARAT—Bring Harmony, Amity, Reconciliation And Trust. Judega BHARAT Jeetega INDIA! https://t.co/L0gsXUEEEK — Jairam Ramesh (@Jairam_Ramesh) September 5, 2023 He added, \"Mr. Modi can continue to distort history and divide India, that is Bharat, that is a Union of States. But we will not be deterred. After all, what is the objective of INDIA parties? It is BHARAT-Bring Harmony, Amity, Reconciliation And Trust. Judega BHARAT. Jeetega INDIA!\" Congress MP Shashi Tharoor said he hoped the government would not be \"foolish\" enough to dispense with the name \\'India\\', which has a brand value built over centuries \"While there is no constitutional objection to calling India \\'Bharat\\', which is one of the country\\'s two official names, I hope the government will not be so foolish as to completely dispense with \"India\", which has incalculable brand value built up over centuries. We should continue to use both words rather than relinquish our claim to a name redolent of history, a name that is recognised around the world,\" he said on X. BJP leaders and ministers, however, welcomed the use of the word Bharat and accused the Congress of being anti-national and anti-constitutional. One of the first reactions was from Assam Chief Minister Himanta Biswa Sarma, who said the wording of the invite made him proud. \"REPUBLIC OF BHARAT - happy and proud that our civilisation is marching ahead boldly towards AMRIT KAAL,\" Mr Sarma said on X. REPUBLIC OF BHARAT - happy and proud that our civilisation is marching ahead boldly towards AMRIT KAAL — Himanta Biswa Sarma (@himantabiswa) September 5, 2023 In a stinging response to the statement by Congress leaders, BJP President JP Nadda said the opposition party has no respect for the country or the Constitution. कांग्रेस को देश के सम्मान एवं गौरव से जुड़े हर विषय से इतनी आपत्ति क्यों है? भारत जोड़ो के नाम पर राजनीतिक यात्रा करने वालों को “भारत माता की जय” के उद्घोष से नफरत क्यों है? स्पष्ट है कि कांग्रेस के मन में न देश के प्रति सम्मान है, न देश के संविधान के प्रति और न ही संवैधानिक… — Jagat Prakash Nadda (@JPNadda) September 5, 2023 \"Why does the Congress object so much to every issue related to the honour and pride of the country? Why does a party that took out a political yatra in the name of Bharat Jodo hate the proclamation of Bharat Mata Ki Jai? It is clear that the Congress has no respect for the country, Constitution, or constitutional institutions. All it cares about is the praise of one particular family. The whole country knows the anti-national and anti-constitutional intentions of the Congress very well,\" the BJP chief said in Hindi. Speaking to news agency ANI, Union Minister Dharmendra Pradhan said the decision to use \\'Bharat\\' is a big statement against the colonial mindset. \"This should have happened earlier. It gives me great satisfaction. \\'Bharat\\' is our introduction and we are proud of it. The President has given priority to \\'Bharat\\'. I am certain the country will be very happy on learning of this. This is a big statement against the colonial mindset,\" he said. A one-line post by megastar Amitabh Bachchan was also linked by social media users to the political row over the G20 invite. T 4759 - 🇮🇳 भारत माता की जय 🚩 — Amitabh Bachchan (@SrBachchan) September 5, 2023 \"Bharat Mata Ki Jai (victory to Mother India),\" the actor posted on X.',\n",
       "  'image_url': 'https://c.ndtvimg.com/2023-08/r4cgkh0g_arvind-kejriwal-satna-madhya-pradesh_625x300_20_August_23.jpg',\n",
       "  'summary': 'The Congress has accused the BJP government of \"distorting history and dividing India\". The use of \\'President of Bharat\\' in place of the traditional \\'President of India\\' in an official invitation to foreign leaders attending the G20 summit has sparked a flurry of political reactions. While opposition leaders have slammed the move and linked it to their 28-party alliance naming itself INDIA, the BJP has questioned why some parties \"object to every issue related to the honour and pride of the country\".',\n",
       "  'title': \"How can BJP strike down 'INDIA': Congress on 'President of Bharat' invitation\"}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
