{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install --disable-pip-version-check \\\n",
    "#     torch==1.13.1 \\\n",
    "#     torchdata==0.5.1 --quiet\n",
    "\n",
    "# %pip install \\\n",
    "#     transformers==4.27.2 \\\n",
    "#     datasets==2.11.0 \\\n",
    "#     evaluate==0.4.0 \\\n",
    "#     rouge_score==0.1.2 \\\n",
    "#     loralib==0.1.1 \\\n",
    "#     peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 29 06:32:10 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:0C:00.0 Off |                  Off |\n",
      "| 30%   34C    P2    65W / 300W |   2481MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:0D:00.0 Off |                  Off |\n",
      "| 30%   25C    P8    31W / 300W |   3899MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:0E:00.0 Off |                  Off |\n",
      "| 30%   24C    P8    20W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:0F:00.0 Off |                  Off |\n",
      "| 30%   23C    P8    14W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-09-03 13:23:19.573288: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-03 13:23:21.056797: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-03 13:23:21.056953: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-03 13:23:21.056969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForSeq2SeqLM, \n",
    "                          AutoTokenizer, \n",
    "                          GenerationConfig, \n",
    "                          TrainingArguments, \n",
    "                          Trainer)\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from saar.models.instruct import PeftModelUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"summary\"\n",
    "\n",
    "assert MODE in [\"summary\", \"title\", \"summary and title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original model\n",
    "name='google/flan-t5-base'\n",
    "model, tokenizer = PeftModelUtils.load_base_model(model_path=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inshorts_scraped.json\n"
     ]
    }
   ],
   "source": [
    "# load and aggregate raw data\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Specify the folder path containing the JSON files\n",
    "folder_path = './data'\n",
    "\n",
    "# Initialize an empty list to aggregate the data\n",
    "data = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json') and \"shorts\" in filename:\n",
    "        print(filename)\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read and parse JSON data from the file\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            file_data = json.load(json_file)\n",
    "            \n",
    "            # Assuming each JSON file contains a list of dictionaries\n",
    "            if isinstance(file_data, list):\n",
    "                data.extend(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Man accused of sexually assaulting daughter granted bail by HC amid matrimonial dispute',\n",
       " 'summary': 'Delhi HC granted bail to a man accused of sexually assaulting his daughter, noting that it cannot shut its eyes to matrimonial dispute between her parents and his false implication by \"tutoring\" cannot be ruled out. It observed she has been residing with the mother for over four years. The court also noted there was inordinate delay in FIR registration.',\n",
       " 'link': 'https://www.outlookindia.com/national/delhi-hc-grants-bail-to-man-accused-of-sexually-assaulting-daughter-news-311110/amp?utm_campaign=fullarticle&utm_medium=referral&utm_source=inshorts',\n",
       " 'image_link': 'https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2023/08_aug/16_wed/img_1692206351958_642.jpg?',\n",
       " 'source': 'inshorts',\n",
       " 'full_text': 'Delhi HC Grants Bail To Man Accused Of Sexually Assaulting Daughter Justice Vikas Mahajan observed the girl has been residing with the mother for more than 4 years and there was an inordinate delay in the registration of the FIR. Delhi High Court The Delhi High Court has granted bail to a man accused of sexually assaulting his daughter while noting that it cannot shut its eyes to the matrimonial dispute between the girl’s parents and his false implication by \"tutoring\" cannot be ruled out. Justice Vikas Mahajan observed the girl has been residing with the mother for more than 4 years and there was an inordinate delay in the registration of the FIR. It noted there were many cross FIRs from the mother\\'s as well as the father\\'s side and \"there is not an iota of reference\" to the alleged incidents of sexual assault in the earlier complaints by the mother. \"Undisputedly, the allegations are serious, but this court cannot shut its eyes to the fact that there is a matrimonial dispute pending between the victim’s parents...In this factual backdrop, false implication of the petitioner by the complainant by tutoring the minor girl child who is in complainant’s custody, cannot be ruled out,\" said the court in a recent order. \"I am prima facie of the view that the above factors have the potential of creating dent in the case of the prosecution,\" the court said. The petitioner father, while seeking bail, told the court there was martial discord between him and his wife, and while the girl aged about 15 years was residing with her mother, a minor son of 10 years was in his care and custody. He alleged his wife was living with a police officer who was helping her file frivolous and bogus complaints against the petitioner. The petitioner was arrested on February 21 and sent to judicial custody. Noting that the incidents alleged occurred in 2019-2022, the complaint was made for the first time only in 2023, the court said that \"evidently, there is an inordinate delay in the registration of FIR\". The court observed the objective of keeping a person in custody is to ensure his availability to face the trial and to receive the sentence that may be awarded and that detention is not supposed to be a punitive or preventive measure. The accused cannot be kept in custody for an indefinite period if trial is not likely to be concluded within reasonable time, it said. In the present case, the court said, the investigation is complete and charge-sheet has been filed but the conclusion of trial was likely to take time. \"In the given circumstances, no useful purpose will be served in keeping the petitioner behind bars...Accordingly, the petitioner is admitted to bail subject to his furnishing a Personal Bond in the sum of Rs.25,000/- and one Surety Bond of the like amount subject to the satisfaction of the Trial Court / Jail Superintendent/Duty Magistrate,\" ordered the court. The court asked the petitioner to not communicate with or establish contact with the alleged victim or witnesses. -With PTI Input',\n",
       " 'createdAt': nan}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# curate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22477"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "data = [news for news in data if news[\"full_text\"] != \"\" and \"JavaScript is not available\" not in news[\"full_text\"] and \"reuters\" not in news[\"link\"]]\n",
    "random.shuffle(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for news in data:\n",
    "    if \"<p>\" in news[\"summary\"]:\n",
    "        # Regular expression to match content between <p> tags\n",
    "        pattern = re.compile(r'<p>(.*?)</p>', re.DOTALL)\n",
    "        matches = pattern.findall(news[\"summary\"])\n",
    "\n",
    "        # Extracted content from <p> tags\n",
    "        extracted_content = [re.sub(r'<.*?>', '', match) for match in matches]\n",
    "        news[\"summary\"] = max(extracted_content, key=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # define the prompt functions\n",
    "        prompt_functions = []\n",
    "\n",
    "        if \"summary\" in MODE:\n",
    "            prompt_functions.append(self._get_summary_prompt)\n",
    "\n",
    "        if \"title\" in MODE:\n",
    "            prompt_functions.append(self._get_title_prompt)\n",
    "\n",
    "        # get prompts\n",
    "        for function in prompt_functions:\n",
    "            for news in data:\n",
    "                input_prompt, label = function(news)\n",
    "                self.inputs.append(input_prompt)\n",
    "                self.labels.append(label)\n",
    "            \n",
    "        \"\"\"\n",
    "        Combine the lists using zip\n",
    "        Shuffle the combined list\n",
    "        Unpack the shuffled pairs back into separate lists\n",
    "        And then tokenize\n",
    "        \"\"\"\n",
    "        combined = list(zip(self.inputs, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.inputs, self.labels = zip(*combined)\n",
    "\n",
    "        # tokenize\n",
    "        self.inputs = tokenizer(self.inputs, \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\").input_ids\n",
    "\n",
    "        self.labels = tokenizer(self.labels, \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\").input_ids\n",
    "            \n",
    "    def __len__(self): \n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_summary_prompt(example):\n",
    "        # word count round off\n",
    "        multiple = 25\n",
    "        word_count = len(example[\"summary\"].split())\n",
    "        word_count = int(round(word_count / multiple)) * multiple\n",
    "        \n",
    "        start_prompt = f'Summarize this news article in {word_count} words.\\n\\n'\n",
    "        end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "        prompt = start_prompt + example[\"full_text\"] + end_prompt\n",
    "\n",
    "        return prompt, example[\"summary\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_title_prompt(example):\n",
    "        # word count round off\n",
    "        multiple = 5\n",
    "        word_count = len(example[\"title\"].split())\n",
    "        word_count = int(ceil(word_count / multiple)) * multiple\n",
    "        \n",
    "        start_prompt = f'Give a title to the given news article in not more than {word_count} words.\\n\\n'\n",
    "        mid_prompt = '\\n\\nSummary: '\n",
    "        end_prompt = '\\n\\nTitle: '\n",
    "\n",
    "        prompt = start_prompt + example[\"full_text\"] + mid_prompt + example[\"summary\"] + end_prompt\n",
    "        return prompt, example[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TextDataset(data, tokenizer)\n",
    "# test_data = TextDataset(data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6434,     3,     9,  2233,    12,     8,   787,  1506,  1108,    16,\n",
       "            59,    72,   145,   627,  1234,     5,  1547,    43,  2031,  3030,\n",
       "            12,  3197,    80,   223,    16,     8,   874,    18, 19515,   332,\n",
       "          1755,   196,   939,   227,   352,     3,   632,  4949,   323,   778,\n",
       "            16,     8,  2101,   192,  1031,     5,   461,  2818,     6,    44,\n",
       "             8,   337,  5669,   116,    79,  8151,    70,   511,  2541,  1453,\n",
       "             6,  1547,  3030,    12,  4943,    46,  8943,   547,   447,  2391,\n",
       "            18,  5981,    15,    17,  1369,   213,  3705,    63, 16296,  1635,\n",
       "          4701,    26,     9,   208,     3, 25202,    15,    26,   223,    12,\n",
       "           607,    16,   869,    12,   428,     3,     9, 15937,    13,   112,\n",
       "            20,   900, 11026,   200,    16,     8,  1910,     5,   978, 22716,\n",
       "          7673,    13,     3,  4591,  3154,   326,  8537, 11607,  2139,  1547,\n",
       "         15389,   323,     8,  2387,    13, 11321,    28,  1179, 11607,  8179,\n",
       "             5,   299,     3,    88,  2088,    31,    17,     8,  4199,  1053,\n",
       "          1187,     8,  1369,     5,  5209,   332,   173,  1639,   584, 12764,\n",
       "             6,   113,    65,   118,     8,   163,  1465,    45,  1547,    31,\n",
       "             7,   332,  1755,   196,   939,   581,  1244,    86,  7719,     6,\n",
       "          2925,   112, 25781,   607,    12,  2604,    46,    73, 17349,  9526,\n",
       "             5,   611,     6,  2675,   130,   646, 11349,    53,    44,     8,\n",
       "           414,    13,     8,  3805,  1369,    38,    79,  7774, 14268,  3504,\n",
       "         23692,   276,   232,    63,     9,    21,   112,    96, 27826,    11,\n",
       "         12447,   924,   121,  1810,    12,   177,    63,     8,  1021,  1370,\n",
       "             3,     9,   168,    18, 21348,   511, 12096, 18358,    16,     8,\n",
       "           939,     5,   332,   173,  1639,   584, 12764,  4743,   448,    61,\n",
       "            11,  3504, 23692,   276,   232,    63,     9,    41,   448,   201,\n",
       "            13,  1547,     6,   227,  3447,     8,  1025,   332,  1755,   196,\n",
       "         18096,  1588,   344,  1547,    11,  1244,    86,  7719,    44,  2846,\n",
       "         21247,   868,  1061,    16, 29218,     6,  2846, 21247,   599, 26487,\n",
       "            61,     3,     2, 10041, 25252, 22573,     2,     3,     2,    87,\n",
       "         10041, 25252, 22573,     2,     3,     2, 10041, 25252, 22573,     2,\n",
       "             3,     2,    87, 10041, 25252, 22573,     2, 14490,   112,   194,\n",
       "           139,     8,  1547, 12025,    30,     8,   223,    13,   192, 22716,\n",
       "          9385,    21, 15810,  2557,     7,    16,    27,  5329,     6,   332,\n",
       "           173,  1639,    47, 14014,   112,  1038,  5695,    16,     8,   166,\n",
       "          1588,    13,     8,   332,  1755,   196,   939,   213,     3,    88,\n",
       "          5799,    46,  4423,   944,    18,  3184,  6352,     6,     8,   200,\n",
       "          2604,    57,    46,  2557, 15839,    16,     8,  1588,    16,    46,\n",
       "          2904, 12082,     3, 27759,   504,    45,     8,   372,     5,   216,\n",
       "           258,   856,    15,    26,   112,   821,    28,     3,     9,   187,\n",
       "           537,   332,  1755,   196, 18358,    16,     8,   511,   467,    30,\n",
       "             8,     3,     7,    40, 13917,  1273,  2846, 21247,  6242,     5,\n",
       "           461,  2818,     6,     3,    88,     3,  3834,  3510,  3705,    63,\n",
       "         16296,  1635,    16,     3,     9,  1588,    18,    60,  7003,    53,\n",
       "             3,  4225,    18,  4312,  1518,    21,     8,  1025, 29719,   274,\n",
       "             3,    17,    15,    15,    53,   326,    28,   662, 11814,    11,\n",
       "             3,     9,  1296,    12,  2604,    46,    73, 17349,  9526,   326,\n",
       "          6862,     5,    86,   685,     3,    88,  2299,   824,    12,  2604,\n",
       "             3,     9,   511, 12096, 18358,    16,     8,   939,     6,   274,\n",
       "          2684,    53,   323,  1587,     8,   414,    13,  1547,    31,     7,\n",
       "         15389,     5,     3,     2, 10041, 25252, 22573,     2,     3,     2,\n",
       "            87, 10041, 25252, 22573,     2,     3,     2, 10041, 25252, 22573,\n",
       "             2,     1]),\n",
       " tensor([ 3504, 23692,   276,   232,    63,     9, 25669,    15,     7,  1296,\n",
       "            12,   177,    63,   332,   173,  1639,     3,     9,  1253,    12,\n",
       "          1560, 18358,     6,  2675,  8922,     1,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from saar.utils import print_number_of_trainable_model_parameters\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FULL MODEL TRAINING\n",
    "# EPOCH = 1\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#                                   save_steps=5000,\n",
    "#                                   warmup_steps=10,\n",
    "#                                   logging_steps=100,\n",
    "#                                   weight_decay=0.01,\n",
    "#                                   num_train_epochs=EPOCH,\n",
    "#                                   logging_dir='./logs',\n",
    "#                                   output_dir='./checkpoint',\n",
    "#                                   per_device_eval_batch_size=32,\n",
    "#                                   per_device_train_batch_size=32)\n",
    "\n",
    "# Trainer(model=model,\n",
    "#         args=training_args,\n",
    "#         eval_dataset=test_data,\n",
    "#         train_dataset=train_data,\n",
    "#         data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]), \n",
    "#                                     'labels': torch.stack([f[1] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28100' max='28100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28100/28100 2:42:32, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.084500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28100, training_loss=0.23320891139346087, metrics={'train_runtime': 9754.7113, 'train_samples_per_second': 23.042, 'train_steps_per_second': 2.881, 'total_flos': 1.5635652772626432e+17, 'train_loss': 0.23320891139346087, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PEFT MODEL TRAINING\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "EPOCH = 10\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))\n",
    "\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "                                  # save_steps=5000,\n",
    "                                  save_strategy=\"no\",\n",
    "                                  warmup_steps=10,\n",
    "                                  logging_steps=1000,\n",
    "                                  weight_decay=0.01,\n",
    "                                  num_train_epochs=EPOCH,\n",
    "                                  logging_dir='./logs',\n",
    "                                  output_dir='./checkpoint',\n",
    "                                  learning_rate=0.0001,\n",
    "                                  auto_find_batch_size=True)\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "                model=peft_model,\n",
    "                args=peft_training_args,\n",
    "                train_dataset=train_data,\n",
    "                data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]), \n",
    "                                            'labels': torch.stack([f[1] for f in data])})\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save peft adapter\n",
    "adapter_path = \"./checkpoint/title_adapter/\"\n",
    "PeftModelUtils.save_peft_adapter(model=peft_model, model_path=adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge peft with main model and save the model\n",
    "model_path = \"./checkpoint/\"\n",
    "PeftModelUtils.merge_peft_and_save(model=peft_model, model_path=model_path)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
