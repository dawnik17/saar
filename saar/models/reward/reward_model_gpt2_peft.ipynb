{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c988b0c2-6363-48a2-92e8-118ba75047a3",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5b031e-5a70-478c-9f6f-253a03f424d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "\n",
    "save_pkl = lambda data, filepath: pickle.dump(data, open(filepath, \"wb\"))\n",
    "load_pkl = lambda filepath: pickle.load(open(filepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c85e0062-1467-437b-af01-0650b17b8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and aggregate raw data\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Specify the folder path containing the JSON files\n",
    "folder_path = './data'\n",
    "\n",
    "# files read\n",
    "files_read_path = \"./rank_data/files_read.pkl\"\n",
    "files_read = load_pkl(files_read_path) if os.path.exists(files_read_path) else set()\n",
    "current_files_read = set()\n",
    "\n",
    "# Initialize an empty list to aggregate the data\n",
    "data = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json') and filename not in files_read:\n",
    "        current_files_read.add(filename)\n",
    "        \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read and parse JSON data from the file\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            file_data = json.load(json_file)\n",
    "            \n",
    "            # Assuming each JSON file contains a list of dictionaries\n",
    "            if isinstance(file_data, list):\n",
    "                data.extend(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc488aa-f75e-43ab-b110-5efeb8dfc926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'08-16-23.json',\n",
       " '08-17-23.json',\n",
       " '08-18-23.json',\n",
       " '08-19-23.json',\n",
       " '08-20-23.json'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_files_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4df8c-04bf-4f63-8be3-54e3b3c7c3a9",
   "metadata": {},
   "source": [
    "# Curate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4fd41f7-4859-4bd4-af41-07f7ddc53bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51579"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "data = [news for news in data if news[\"full_text\"] != \"\" and \"JavaScript is not available\" not in news[\"full_text\"] and \"reuters\" not in news[\"link\"]]\n",
    "random.shuffle(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58191860-9fba-4c7a-908b-b03156515ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in data:\n",
    "    if \"<p>\" in news[\"summary\"]:\n",
    "        # Regular expression to match content between <p> tags\n",
    "        pattern = re.compile(r'<p>(.*?)</p>', re.DOTALL)\n",
    "        matches = pattern.findall(news[\"summary\"])\n",
    "\n",
    "        # Extracted content from <p> tags\n",
    "        extracted_content = [re.sub(r'<.*?>', '', match) for match in matches]\n",
    "        news[\"summary\"] = max(extracted_content, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a6afa1-1cc1-4794-a0e4-281722776680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Sood Charity Foundation fulfilling my dream of becoming teacher: Sonu',\n",
       " 'summary': 'Sonu Sood said if he wasn\\'t an actor, he would have been a teacher. He said his dream is now coming true with \\'Sood Charity Foundation\\', which is helping many students to pursue education. Calling his mother his \"favourite teacher\", Sonu shared, \"Since my mother was a teacher herself, there was no excuse for me to have secured less marks.\"',\n",
       " 'link': 'https://www.mid-day.com/amp/entertainment/bollywood-news/article/my-teachers-have-taught-me-to-be-patient-and-perseverant-says-sonu-sood-23244355?utm_campaign=fullarticle&utm_medium=referral&utm_source=inshorts',\n",
       " 'image_link': 'https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2022/09_sep/5_mon/img_1662373782807_419.jpg?',\n",
       " 'source': 'inshorts',\n",
       " 'full_text': \"My teachers have taught me that patience and perseverance always do wonders in everyone’s life, says Sonu Sood My teachers have taught me that patience and perseverance always do wonders in everyone’s life, says Sonu Sood He exclusively spoke to us about the importance of Teacher`s Day, amongst other things Official Instagram Account of Sonu Sood The pandemic saw the reel-life actor Sonu Sood turning a real-life hero. Besides facilitating the travel of the needy to meet their loved ones, providing free education, medical help and much moreâ\\x80¦ Sood lent a helping hand to many. He was even nicknamed as â\\x80\\x98Messiah' in lieu of the help which he rendered during the pandemic. Mid-Day Online caught up with Sonu Sood for an exclusive interview on Teacher's Day. Since Sonu's mother was a teacher herself, Sonu spoke about the importance of the day, besides jogging down memory lane. Who was your favourite teacher in school or college and why? My favourite teacher has always been my mother. What is that one thing you have learnt from your teachers, that will remain with you forever? I have learnt that, one should never lose patience and to be perseverant in life. My teachers have taught me that, patience and perseverance always do wonders in anyone's life. These two factors are extremely important to reach one's goals. Which was your favourite subject and least favourite subject in school? My most favourite subject was Mathematics of course, while the least favourite was Geography. If you were not an actor, would you have become a teacher? I would have surely done that. But, through the medium of â\\x80\\x98Sood Charity Foundation', we are helping lots of students to pursue their education. This way, the dream of becoming a teacher is getting fulfilled even after becoming an actor. Also Read: Blackpink become artiste with highest subscribers on Youtube, 'Pink Venom' excluded from music bank How were you as a student in school? Were you a front bencher or a last bencher? I was good student, if not great. I was a last bencher during my ninth and tenth standard. I was always trying to do my bit. Since my mother was a teacher herself, there was no excuse for me to have secured less marks. Have you ever been punished in school by your teachers? Yes, many a times. Those times, the punishments used to be kneeling down outside the class. I had been punished this way do a couple of times during my school days. Is there any punishment that you will remember for the rest of your life? Yes! During my school days, three students (including me) were told to stand outside the class as a punishment. On the pretext of going outside the class, we simply vanished from there and went to watch a movie! We were called the next day and were given a good piece of scolding and beating for that. After that, we never dared to do anything of that sort! Play Quiz: Is this the first time Ranbir Kapoor and Alia Bhatt are paired together?\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fd541-1d11-4ac4-adc2-cfc7bb017942",
   "metadata": {},
   "source": [
    "# Load Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05414b36-ab52-4eaa-81d5-5d98586faf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-08-21 08:29:25.764456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 08:29:27.215233: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-21 08:29:27.215372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-21 08:29:27.215388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, \n",
    "                          AutoTokenizer, \n",
    "                          GenerationConfig, \n",
    "                          TrainingArguments, \n",
    "                          Trainer)\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e555667e-0354-4fd7-8cc1-cfe6a3eeb4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53667694-185c-4108-ad74-e790f48ac64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeftModel:\n",
    "    @staticmethod\n",
    "    def load_base_model(model_path=\"google/flan-t5-base\"):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.bfloat16\n",
    "        ).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_peft_adapter(\n",
    "        base_model_path, peft_model_path, train=False\n",
    "    ):\n",
    "        model, tokenizer = self.load_base_model(base_model_path)\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model, peft_model_path, torch_dtype=torch.bfloat16, is_trainable=False\n",
    "        ).to(device)\n",
    "\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "        if train:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # merge the adapter to the main model\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def save_peft_adapter(model, tokenizer, model_path):\n",
    "        model.save_pretrained(model_path)\n",
    "        tokenizer.save_pretrained(model_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_peft_and_save(model, tokenizer, model_path):\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(model_path)\n",
    "        tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c6bbf1-9831-4eb5-86df-51cef9cbca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load instruct model\n",
    "name = './checkpoint/'\n",
    "model, tokenizer = PeftModel.load_base_model(model_path=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214fb72-202a-41fd-b093-57a188da5b84",
   "metadata": {},
   "source": [
    "# Training Data Generation\n",
    "### with instruct model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669b468c-02a0-47fd-b5e9-2a2af810a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "def get_summary_prompt(example):\n",
    "    # word count round off\n",
    "    multiple = 25\n",
    "    word_count = len(example[\"summary\"].split())\n",
    "    word_count = int(round(word_count / multiple)) * multiple\n",
    "\n",
    "    start_prompt = f'Summarize this news article in {word_count} words.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "    prompt = start_prompt + example[\"full_text\"] + end_prompt\n",
    "\n",
    "    return prompt, example[\"summary\"]\n",
    "\n",
    "\n",
    "def get_title_prompt(example):\n",
    "    # word count round off\n",
    "    multiple = 5\n",
    "    word_count = len(example[\"title\"].split())\n",
    "    word_count = int(ceil(word_count / multiple)) * multiple\n",
    "\n",
    "    start_prompt = f'Give a title to the given news article in not more than {word_count} words.\\n\\n'\n",
    "    mid_prompt = '\\n\\nSummary: '\n",
    "    end_prompt = '\\n\\nTitle: '\n",
    "\n",
    "    prompt = start_prompt + example[\"full_text\"] + mid_prompt + example[\"summary\"] + end_prompt\n",
    "    return prompt, example[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aefb8147-f2f0-490f-8fd5-9a82ad0cc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching data\n",
    "batch = lambda data, batch_size: [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "batch_size = 200\n",
    "batches = batch(data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e768279-14bf-41aa-aa0d-a0be49f62ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 135/258 [33:08<30:12, 14.73s/it] "
     ]
    }
   ],
   "source": [
    "# batched generation\n",
    "training_data = []\n",
    "\n",
    "for news_batch in tqdm(batches):\n",
    "    prompt_batch = []\n",
    "    human_label_batch = []\n",
    "    \n",
    "    for news in news_batch:\n",
    "        for prompt, human_label in [get_summary_prompt(news), get_title_prompt(news)]:\n",
    "            prompt_batch.append(prompt)\n",
    "            human_label_batch.append(human_label)\n",
    "    \n",
    "    input_ids = tokenizer(prompt_batch, return_tensors=\"pt\", truncation=True, padding=True).input_ids.to(\"cuda\")\n",
    "\n",
    "    model_outputs = model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    model_text_output_batch = [tokenizer.decode(model_outputs[i], skip_special_tokens=True) for i in range(len(model_outputs))]\n",
    "    \n",
    "    training_data.extend(list(zip(prompt_batch, human_label_batch, model_text_output_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176d582-2e9c-46ef-a20c-36e23bcb98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6cca0-1e99-427c-bc2f-a8439fe1148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "reward_model_training_data_path = \"./rank_data/data.pkl\"\n",
    "data_store = load_pkl(filepath=reward_model_training_data_path) if os.path.exists(reward_model_training_data_path) else []\n",
    "\n",
    "data_store.extend(training_data)\n",
    "save_pkl(data=data_store, filepath=reward_model_training_data_path)\n",
    "\n",
    "# update files read\n",
    "files_read = files_read.union(current_files_read)\n",
    "save_pkl(files_read, filepath=\"./rank_data/files_read.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffda127-6e9e-49cd-b79f-6ece4ac0a252",
   "metadata": {},
   "source": [
    "# Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c60468-5142-4bc2-8107-02afbb39810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 22 12:01:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:0C:00.0 Off |                  Off |\n",
      "| 30%   25C    P8    23W / 300W |   2325MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:0D:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    31W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:0E:00.0 Off |                  Off |\n",
      "| 48%   72C    P2   235W / 300W |  33559MiB / 48685MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:0F:00.0 Off |                  Off |\n",
      "| 54%   80C    P2   287W / 300W |   5133MiB / 48685MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78313c0f-2e43-4ce9-ac06-d1a0bce6697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "save_pkl = lambda data, filepath: pickle.dump(data, open(filepath, \"wb\"))\n",
    "load_pkl = lambda filepath: pickle.load(open(filepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c5fd30-d5cd-4cc2-a8b5-1e9bddc79d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data/training/inshorts.json', 'r') as json_file:\n",
    "    training_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a5c6fb-4e63-46b6-9a9a-3e2315ddde2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Man accused of sexually assaulting daughter granted bail by HC amid matrimonial dispute',\n",
       " 'summary': 'Delhi HC granted bail to a man accused of sexually assaulting his daughter, noting that it cannot shut its eyes to matrimonial dispute between her parents and his false implication by \"tutoring\" cannot be ruled out. It observed she has been residing with the mother for over four years. The court also noted there was inordinate delay in FIR registration.',\n",
       " 'link': 'https://www.outlookindia.com/national/delhi-hc-grants-bail-to-man-accused-of-sexually-assaulting-daughter-news-311110/amp?utm_campaign=fullarticle&utm_medium=referral&utm_source=inshorts',\n",
       " 'image_link': 'https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2023/08_aug/16_wed/img_1692206351958_642.jpg?',\n",
       " 'source': 'inshorts',\n",
       " 'full_text': 'Delhi HC Grants Bail To Man Accused Of Sexually Assaulting Daughter Justice Vikas Mahajan observed the girl has been residing with the mother for more than 4 years and there was an inordinate delay in the registration of the FIR. Delhi High Court The Delhi High Court has granted bail to a man accused of sexually assaulting his daughter while noting that it cannot shut its eyes to the matrimonial dispute between the girl’s parents and his false implication by \"tutoring\" cannot be ruled out. Justice Vikas Mahajan observed the girl has been residing with the mother for more than 4 years and there was an inordinate delay in the registration of the FIR. It noted there were many cross FIRs from the mother\\'s as well as the father\\'s side and \"there is not an iota of reference\" to the alleged incidents of sexual assault in the earlier complaints by the mother. \"Undisputedly, the allegations are serious, but this court cannot shut its eyes to the fact that there is a matrimonial dispute pending between the victim’s parents...In this factual backdrop, false implication of the petitioner by the complainant by tutoring the minor girl child who is in complainant’s custody, cannot be ruled out,\" said the court in a recent order. \"I am prima facie of the view that the above factors have the potential of creating dent in the case of the prosecution,\" the court said. The petitioner father, while seeking bail, told the court there was martial discord between him and his wife, and while the girl aged about 15 years was residing with her mother, a minor son of 10 years was in his care and custody. He alleged his wife was living with a police officer who was helping her file frivolous and bogus complaints against the petitioner. The petitioner was arrested on February 21 and sent to judicial custody. Noting that the incidents alleged occurred in 2019-2022, the complaint was made for the first time only in 2023, the court said that \"evidently, there is an inordinate delay in the registration of FIR\". The court observed the objective of keeping a person in custody is to ensure his availability to face the trial and to receive the sentence that may be awarded and that detention is not supposed to be a punitive or preventive measure. The accused cannot be kept in custody for an indefinite period if trial is not likely to be concluded within reasonable time, it said. In the present case, the court said, the investigation is complete and charge-sheet has been filed but the conclusion of trial was likely to take time. \"In the given circumstances, no useful purpose will be served in keeping the petitioner behind bars...Accordingly, the petitioner is admitted to bail subject to his furnishing a Personal Bond in the sum of Rs.25,000/- and one Surety Bond of the like amount subject to the satisfaction of the Trial Court / Jail Superintendent/Duty Magistrate,\" ordered the court. The court asked the petitioner to not communicate with or establish contact with the alleged victim or witnesses. -With PTI Input',\n",
       " 'createdAt': nan,\n",
       " 'generated_summary': 'Delhi High Court has granted bail to a man allegedly accused of sexually assaulting his daughter in Delhi while noting that it cannot shut its eyes to the matrimonial dispute between the girl\\'s parents and his false implication by \"tutoring\" cannot be ruled out. Justice Vikas Mahajan observed the girl has been residing with the mother for more than 4 years and there was an inordinate delay in the registration of the FIR. It noted there were many cross FIRs from the mother\\'s as well as the father\\'s side and \"there is not an iota of reference\" to the alleged incidents of sexual assault in the earlier complaints by the mother.',\n",
       " 'generated_title': \"Man accused of sexually assaulting daughter gets bail, says 'false implication' can't be ruled out\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be6859e-138e-4fa5-8578-1e74d83b41d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dawn/git/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from torchinfo import summary\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c46b001-5e6c-449b-8d2d-043c1ba76ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164eb30d-338b-4bdf-a459-48a8ba3bb098",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1810a31e-6eb0-41cb-a993-c1ab87a01482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.max_len = tokenizer.model_max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.prepare(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def prepare(self, prompts):\n",
    "        print(\"preparing data..\")\n",
    "        features = []\n",
    "        \n",
    "        for data in tqdm(prompts):\n",
    "            feature_i = data[\"full_text\"] + \"Summarize: \\n\" + data[\"summary\"]\n",
    "            feature_j = data[\"full_text\"] + \"Summarize: \\n\" + data[\"generated_summary\"]\n",
    "\n",
    "            # handwritten summary\n",
    "            inputs = self.tokenizer(feature_i,             \n",
    "                                    max_length=self.max_len,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    truncation=True,\n",
    "                                    padding=\"max_length\")\n",
    "    \n",
    "            i_ids = inputs['input_ids']\n",
    "            i_mask = inputs['attention_mask']\n",
    "\n",
    "            # model generated summary\n",
    "            inputs = self.tokenizer(feature_j, \n",
    "                                    truncation=True,\n",
    "                                    max_length=self.max_len,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    padding=\"max_length\")\n",
    "    \n",
    "            j_ids = inputs['input_ids']\n",
    "            j_mask = inputs['attention_mask']\n",
    "    \n",
    "            features.append({\n",
    "                'i_ids': i_ids,\n",
    "                'i_mask': i_mask,\n",
    "                'j_ids': j_ids,\n",
    "                'j_mask': j_mask\n",
    "            })\n",
    "        return features\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "\n",
    "        return {\n",
    "            'i_ids': torch.tensor(data[\"i_ids\"], dtype=torch.long),\n",
    "            'i_mask': torch.tensor(data[\"i_mask\"], dtype=torch.long),\n",
    "            'j_ids': torch.tensor(data[\"j_ids\"], dtype=torch.long),\n",
    "            'j_mask': torch.tensor(data[\"j_mask\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6fa56c1-6859-49b9-b79c-eb5441794c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 22064/22064 [01:07<00:00, 327.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 5516/5516 [00:19<00:00, 286.04it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "train_size = int(len(training_data) * train_ratio)\n",
    "\n",
    "train_data = training_data[:train_size].copy()\n",
    "test_data = training_data[train_size:].copy()\n",
    "\n",
    "training_set = CustomDataset(train_data, tokenizer)\n",
    "testing_set = CustomDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f64d9c0-2f97-4724-bc3c-b1123875bf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i_ids': tensor([13856,  5303, 27327,  ..., 50256, 50256, 50256]),\n",
       " 'i_mask': tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       " 'j_ids': tensor([13856,  5303, 27327,  ..., 50256, 50256, 50256]),\n",
       " 'j_mask': tensor([1, 1, 1,  ..., 0, 0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c5fddc-e152-4371-be8c-632476d8d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 0\n",
    "                }\n",
    "\n",
    "trainloader = DataLoader(training_set, **train_params)\n",
    "testloader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80fe3468-1a0f-44e2-a2c5-725b113f60c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "781927bd-777c-429e-b00f-ba973cf08dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "class RewardModel(torch.nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.l1 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            # task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "\n",
    "        self.l1 = get_peft_model(self.l1, lora_config)\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(768, 1),\n",
    "        )\n",
    "        self.outl = nn.Sigmoid()\n",
    "        \n",
    "    def gpt2(self, ids, attention_mask):\n",
    "        # logits shape: [batch=16, seqlen=1024, dim=768]\n",
    "        logits, _ = self.l1(ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            return_dict=False)\n",
    "        \n",
    "        sequence_lengths = (torch.eq(ids, tokenizer.pad_token_id).long().argmax(-1) - 1).to(\n",
    "                    logits.device\n",
    "                )\n",
    "        \n",
    "        batch_size = ids.shape[0]\n",
    "        \n",
    "        # pooled logits shape: [batch=16, dim=768]\n",
    "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
    "        return pooled_logits\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        ids shape = [batch, 3, 512]\n",
    "        3: prompt, human_text, model_text\n",
    "        \"\"\"  \n",
    "        i = self.gpt2(input[\"i_ids\"], attention_mask=input[\"i_mask\"])\n",
    "        j = self.gpt2(input[\"j_ids\"], attention_mask=input[\"j_mask\"])\n",
    "        \n",
    "        i = self.l2(i)\n",
    "        j = self.l2(j)\n",
    "        \n",
    "        return self.outl(i - j)\n",
    "    \n",
    "    def predict(self, input):\n",
    "        \"\"\"\n",
    "        ids shape: [batch, 2, 512]\n",
    "        2: prompt, text\n",
    "        \"\"\"\n",
    "        i = self.gpt2(input[\"ids\"], attention_mask=input[\"mask\"])\n",
    "        return self.l2(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fccd9835-980a-43b3-b049-353baf696677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dawn/git/venv/lib/python3.11/site-packages/peft/tuners/lora.py:240: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = RewardModel(dropout=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ae7408-d126-48d6-bb79-38bf05e762ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "RewardModel                                                  [2, 1]                    --\n",
       "├─PeftModel: 1-1                                             [2, 1024, 768]            --\n",
       "│    └─LoraModel: 2-2                                        --                        (recursive)\n",
       "│    │    └─GPT2Model: 3-1                                   [2, 1024, 768]            124,734,720\n",
       "├─PeftModel: 1-2                                             [2, 1024, 768]            (recursive)\n",
       "│    └─LoraModel: 2-2                                        --                        (recursive)\n",
       "│    │    └─GPT2Model: 3-2                                   [2, 1024, 768]            (recursive)\n",
       "├─Sequential: 1-3                                            [2, 1]                    --\n",
       "│    └─Linear: 2-3                                           [2, 1]                    769\n",
       "├─Sequential: 1-4                                            [2, 1]                    (recursive)\n",
       "│    └─Linear: 2-4                                           [2, 1]                    (recursive)\n",
       "├─Sigmoid: 1-5                                               [2, 1]                    --\n",
       "==============================================================================================================\n",
       "Total params: 124,735,489\n",
       "Trainable params: 295,681\n",
       "Non-trainable params: 124,439,808\n",
       "Total mult-adds (Units.GIGABYTES): 457.28\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 3387.95\n",
       "Params size (MB): 413.90\n",
       "Estimated Total Size (MB): 3801.91\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# sample input\n",
    "sample_input = next(iter(trainloader))\n",
    "summary(model, input_data=[sample_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a6f688b-ea03-4059-b095-2c9c4a5c8243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardModel(\n",
       "  (l1): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (l2): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       "  (outl): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3154f425-9f23-4623-a0c3-965222248a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4661d05b-ece1-4611-93cc-aaf546c467e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    \n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader)\n",
    "    \n",
    "    for idx, data in enumerate(pbar):\n",
    "        data[\"i_ids\"] = data[\"i_ids\"].to(device)\n",
    "        data[\"i_mask\"] = data[\"i_mask\"].to(device)\n",
    "        data[\"j_ids\"] = data[\"j_ids\"].to(device)\n",
    "        data[\"j_mask\"] = data[\"j_mask\"].to(device)\n",
    "\n",
    "        batch_size = data[\"i_ids\"].size()[0]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(data)\n",
    "        target = torch.from_numpy(np.ones(shape=(batch_size, 1))).float().to(device)\n",
    "        \n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct += (\n",
    "            sum((pred.detach().squeeze(-1) >= 0.50).float() == target.squeeze(-1))\n",
    "            .detach()\n",
    "            .item()\n",
    "        )\n",
    "        processed += pred.detach().shape[0]\n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "        # tqdm writing\n",
    "        pbar.set_description(\n",
    "            desc=\"Train Epoch: {epoch}, Mini Batch: {batch}, Train Accuracy: {accuracy}, Train Loss: {loss}\".format(\n",
    "                epoch=epoch,\n",
    "                batch=idx+1,\n",
    "                accuracy=round((correct / processed) * 100, 4),\n",
    "                loss=round(total_loss / (idx+1), 4)\n",
    "            )\n",
    "        )\n",
    "    return round((correct / processed) * 100, 4), round(total_loss / (idx+1), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7a050a-2527-428a-be9e-9fa0568785b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(pbar):\n",
    "            data[\"i_ids\"] = data[\"i_ids\"].to(device)\n",
    "            data[\"i_mask\"] = data[\"i_mask\"].to(device)\n",
    "            data[\"j_ids\"] = data[\"j_ids\"].to(device)\n",
    "            data[\"j_mask\"] = data[\"j_mask\"].to(device)\n",
    "\n",
    "            batch_size = data[\"i_ids\"].size()[0]\n",
    "\n",
    "            pred = model(data)\n",
    "            target = torch.from_numpy(np.ones(shape=(batch_size, 1))).float().to(device)\n",
    "\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            correct += (\n",
    "                sum((pred.detach().squeeze(-1) >= 0.50).float() == target.squeeze(-1))\n",
    "                .detach()\n",
    "                .item()\n",
    "            )\n",
    "            processed += pred.detach().shape[0]\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "            # tqdm writing\n",
    "            pbar.set_description(\n",
    "                desc=\"Test Epoch: {epoch}, Mini Batch: {batch}, Test Accuracy: {accuracy}, Test Loss: {loss}\".format(\n",
    "                    epoch=epoch,\n",
    "                    batch=idx+1,\n",
    "                    accuracy=round((correct / processed) * 100, 4),\n",
    "                    loss=round(total_loss / (idx+1), 4)\n",
    "                )\n",
    "            )\n",
    "        return round((correct / processed) * 100, 4), round(total_loss / (idx + 1), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c79095-b019-443e-8c21-758be55e4f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 295681\n",
      "all model parameters: 124735489\n",
      "percentage of trainable model parameters: 0.24%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    \n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    \n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a69801-ae46-47bf-9d5f-e90f601a19a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Mini Batch: 2716, Train Accuracy: 86.0457, Train Loss: 0.2754:  25%|▏| 2716/11032 [1"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_accuracy, train_loss = train(epoch, model, trainloader, optimizer, criterion, device)\n",
    "    test_accuracy, test_loss = test(epoch, model, testloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b3f2a6d-e2c0-4705-bc2d-7b47dd96fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l1.save_pretrained(\"./reward_model_checkpoint/peft_gpt2/\")\n",
    "torch.save(model.l2.state_dict(), \"./reward_model_checkpoint/peft_gpt2/l2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fcde5-87bf-411a-867e-f99c6812b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_i = PROMPT + HUMAN SUMMARY\n",
    "input_j = PROMPT + MODEL GENERATED SUMMARY\n",
    "\n",
    "SIGMOID(LINEAR(GPT2(input_i)) - LINEAR(GPT2(input_j))) = 1\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
