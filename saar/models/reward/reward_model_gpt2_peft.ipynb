{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c988b0c2-6363-48a2-92e8-118ba75047a3",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5b031e-5a70-478c-9f6f-253a03f424d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "\n",
    "save_pkl = lambda data, filepath: pickle.dump(data, open(filepath, \"wb\"))\n",
    "load_pkl = lambda filepath: pickle.load(open(filepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c85e0062-1467-437b-af01-0650b17b8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and aggregate raw data\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Specify the folder path containing the JSON files\n",
    "folder_path = './data'\n",
    "\n",
    "# files read\n",
    "files_read_path = \"./rank_data/files_read.pkl\"\n",
    "files_read = load_pkl(files_read_path) if os.path.exists(files_read_path) else set()\n",
    "current_files_read = set()\n",
    "\n",
    "# Initialize an empty list to aggregate the data\n",
    "data = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json') and filename not in files_read:\n",
    "        current_files_read.add(filename)\n",
    "        \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read and parse JSON data from the file\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            file_data = json.load(json_file)\n",
    "            \n",
    "            # Assuming each JSON file contains a list of dictionaries\n",
    "            if isinstance(file_data, list):\n",
    "                data.extend(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc488aa-f75e-43ab-b110-5efeb8dfc926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'08-16-23.json',\n",
       " '08-17-23.json',\n",
       " '08-18-23.json',\n",
       " '08-19-23.json',\n",
       " '08-20-23.json'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_files_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4df8c-04bf-4f63-8be3-54e3b3c7c3a9",
   "metadata": {},
   "source": [
    "# Curate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4fd41f7-4859-4bd4-af41-07f7ddc53bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51579"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "data = [news for news in data if news[\"full_text\"] != \"\" and \"JavaScript is not available\" not in news[\"full_text\"] and \"reuters\" not in news[\"link\"]]\n",
    "random.shuffle(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58191860-9fba-4c7a-908b-b03156515ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in data:\n",
    "    if \"<p>\" in news[\"summary\"]:\n",
    "        # Regular expression to match content between <p> tags\n",
    "        pattern = re.compile(r'<p>(.*?)</p>', re.DOTALL)\n",
    "        matches = pattern.findall(news[\"summary\"])\n",
    "\n",
    "        # Extracted content from <p> tags\n",
    "        extracted_content = [re.sub(r'<.*?>', '', match) for match in matches]\n",
    "        news[\"summary\"] = max(extracted_content, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a6afa1-1cc1-4794-a0e4-281722776680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Sood Charity Foundation fulfilling my dream of becoming teacher: Sonu',\n",
       " 'summary': 'Sonu Sood said if he wasn\\'t an actor, he would have been a teacher. He said his dream is now coming true with \\'Sood Charity Foundation\\', which is helping many students to pursue education. Calling his mother his \"favourite teacher\", Sonu shared, \"Since my mother was a teacher herself, there was no excuse for me to have secured less marks.\"',\n",
       " 'link': 'https://www.mid-day.com/amp/entertainment/bollywood-news/article/my-teachers-have-taught-me-to-be-patient-and-perseverant-says-sonu-sood-23244355?utm_campaign=fullarticle&utm_medium=referral&utm_source=inshorts',\n",
       " 'image_link': 'https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2022/09_sep/5_mon/img_1662373782807_419.jpg?',\n",
       " 'source': 'inshorts',\n",
       " 'full_text': \"My teachers have taught me that patience and perseverance always do wonders in everyone’s life, says Sonu Sood My teachers have taught me that patience and perseverance always do wonders in everyone’s life, says Sonu Sood He exclusively spoke to us about the importance of Teacher`s Day, amongst other things Official Instagram Account of Sonu Sood The pandemic saw the reel-life actor Sonu Sood turning a real-life hero. Besides facilitating the travel of the needy to meet their loved ones, providing free education, medical help and much moreâ\\x80¦ Sood lent a helping hand to many. He was even nicknamed as â\\x80\\x98Messiah' in lieu of the help which he rendered during the pandemic. Mid-Day Online caught up with Sonu Sood for an exclusive interview on Teacher's Day. Since Sonu's mother was a teacher herself, Sonu spoke about the importance of the day, besides jogging down memory lane. Who was your favourite teacher in school or college and why? My favourite teacher has always been my mother. What is that one thing you have learnt from your teachers, that will remain with you forever? I have learnt that, one should never lose patience and to be perseverant in life. My teachers have taught me that, patience and perseverance always do wonders in anyone's life. These two factors are extremely important to reach one's goals. Which was your favourite subject and least favourite subject in school? My most favourite subject was Mathematics of course, while the least favourite was Geography. If you were not an actor, would you have become a teacher? I would have surely done that. But, through the medium of â\\x80\\x98Sood Charity Foundation', we are helping lots of students to pursue their education. This way, the dream of becoming a teacher is getting fulfilled even after becoming an actor. Also Read: Blackpink become artiste with highest subscribers on Youtube, 'Pink Venom' excluded from music bank How were you as a student in school? Were you a front bencher or a last bencher? I was good student, if not great. I was a last bencher during my ninth and tenth standard. I was always trying to do my bit. Since my mother was a teacher herself, there was no excuse for me to have secured less marks. Have you ever been punished in school by your teachers? Yes, many a times. Those times, the punishments used to be kneeling down outside the class. I had been punished this way do a couple of times during my school days. Is there any punishment that you will remember for the rest of your life? Yes! During my school days, three students (including me) were told to stand outside the class as a punishment. On the pretext of going outside the class, we simply vanished from there and went to watch a movie! We were called the next day and were given a good piece of scolding and beating for that. After that, we never dared to do anything of that sort! Play Quiz: Is this the first time Ranbir Kapoor and Alia Bhatt are paired together?\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fd541-1d11-4ac4-adc2-cfc7bb017942",
   "metadata": {},
   "source": [
    "# Load Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05414b36-ab52-4eaa-81d5-5d98586faf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-08-21 08:29:25.764456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 08:29:27.215233: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-21 08:29:27.215372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-21 08:29:27.215388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, \n",
    "                          AutoTokenizer, \n",
    "                          GenerationConfig, \n",
    "                          TrainingArguments, \n",
    "                          Trainer)\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e555667e-0354-4fd7-8cc1-cfe6a3eeb4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53667694-185c-4108-ad74-e790f48ac64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeftModel:\n",
    "    @staticmethod\n",
    "    def load_base_model(model_path=\"google/flan-t5-base\"):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.bfloat16\n",
    "        ).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_peft_adapter(\n",
    "        base_model_path, peft_model_path, train=False\n",
    "    ):\n",
    "        model, tokenizer = self.load_base_model(base_model_path)\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model, peft_model_path, torch_dtype=torch.bfloat16, is_trainable=False\n",
    "        ).to(device)\n",
    "\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "        if train:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # merge the adapter to the main model\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def save_peft_adapter(model, tokenizer, model_path):\n",
    "        model.save_pretrained(model_path)\n",
    "        tokenizer.save_pretrained(model_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_peft_and_save(model, tokenizer, model_path):\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(model_path)\n",
    "        tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c6bbf1-9831-4eb5-86df-51cef9cbca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load instruct model\n",
    "name = './checkpoint/'\n",
    "model, tokenizer = PeftModel.load_base_model(model_path=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214fb72-202a-41fd-b093-57a188da5b84",
   "metadata": {},
   "source": [
    "# Training Data Generation\n",
    "### with instruct model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669b468c-02a0-47fd-b5e9-2a2af810a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "def get_summary_prompt(example):\n",
    "    # word count round off\n",
    "    multiple = 25\n",
    "    word_count = len(example[\"summary\"].split())\n",
    "    word_count = int(round(word_count / multiple)) * multiple\n",
    "\n",
    "    start_prompt = f'Summarize this news article in {word_count} words.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "    prompt = start_prompt + example[\"full_text\"] + end_prompt\n",
    "\n",
    "    return prompt, example[\"summary\"]\n",
    "\n",
    "\n",
    "def get_title_prompt(example):\n",
    "    # word count round off\n",
    "    multiple = 5\n",
    "    word_count = len(example[\"title\"].split())\n",
    "    word_count = int(ceil(word_count / multiple)) * multiple\n",
    "\n",
    "    start_prompt = f'Give a title to the given news article in not more than {word_count} words.\\n\\n'\n",
    "    mid_prompt = '\\n\\nSummary: '\n",
    "    end_prompt = '\\n\\nTitle: '\n",
    "\n",
    "    prompt = start_prompt + example[\"full_text\"] + mid_prompt + example[\"summary\"] + end_prompt\n",
    "    return prompt, example[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aefb8147-f2f0-490f-8fd5-9a82ad0cc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching data\n",
    "batch = lambda data, batch_size: [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "batch_size = 200\n",
    "batches = batch(data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e768279-14bf-41aa-aa0d-a0be49f62ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 135/258 [33:08<30:12, 14.73s/it] "
     ]
    }
   ],
   "source": [
    "# batched generation\n",
    "training_data = []\n",
    "\n",
    "for news_batch in tqdm(batches):\n",
    "    prompt_batch = []\n",
    "    human_label_batch = []\n",
    "    \n",
    "    for news in news_batch:\n",
    "        for prompt, human_label in [get_summary_prompt(news), get_title_prompt(news)]:\n",
    "            prompt_batch.append(prompt)\n",
    "            human_label_batch.append(human_label)\n",
    "    \n",
    "    input_ids = tokenizer(prompt_batch, return_tensors=\"pt\", truncation=True, padding=True).input_ids.to(\"cuda\")\n",
    "\n",
    "    model_outputs = model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    model_text_output_batch = [tokenizer.decode(model_outputs[i], skip_special_tokens=True) for i in range(len(model_outputs))]\n",
    "    \n",
    "    training_data.extend(list(zip(prompt_batch, human_label_batch, model_text_output_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176d582-2e9c-46ef-a20c-36e23bcb98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6cca0-1e99-427c-bc2f-a8439fe1148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "reward_model_training_data_path = \"./rank_data/data.pkl\"\n",
    "data_store = load_pkl(filepath=reward_model_training_data_path) if os.path.exists(reward_model_training_data_path) else []\n",
    "\n",
    "data_store.extend(training_data)\n",
    "save_pkl(data=data_store, filepath=reward_model_training_data_path)\n",
    "\n",
    "# update files read\n",
    "files_read = files_read.union(current_files_read)\n",
    "save_pkl(files_read, filepath=\"./rank_data/files_read.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffda127-6e9e-49cd-b79f-6ece4ac0a252",
   "metadata": {},
   "source": [
    "# Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c60468-5142-4bc2-8107-02afbb39810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 22 12:01:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:0C:00.0 Off |                  Off |\n",
      "| 30%   25C    P8    23W / 300W |   2325MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:0D:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    31W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:0E:00.0 Off |                  Off |\n",
      "| 48%   72C    P2   235W / 300W |  33559MiB / 48685MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:0F:00.0 Off |                  Off |\n",
      "| 54%   80C    P2   287W / 300W |   5133MiB / 48685MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78313c0f-2e43-4ce9-ac06-d1a0bce6697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "\n",
    "save_pkl = lambda data, filepath: pickle.dump(data, open(filepath, \"wb\"))\n",
    "load_pkl = lambda filepath: pickle.load(open(filepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c5fd30-d5cd-4cc2-a8b5-1e9bddc79d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_pkl(\"./rank_data/data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a5c6fb-4e63-46b6-9a9a-3e2315ddde2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Summarize this news article in 50 words.\\n\\nA former BJP MLA from Jharkhand took law in his hands and decided to punish a youth for allegedly make videos of women while bathing. In the video, former Jarmundi MLA Devendra Kunwar can be seen telling the youth to do sit-ups. The leader then asks him to spit and lick. As he follows the instructions of the former legislator, Kunwar kicks him. The youth can be seen weeping in the video. The inhumane punishment was given at the ‘kangaroo court’ of Kunwar, during a gram sabha meeting. Some of the spectators could be seen cheering and recording the video. Reacting to the viral video, Kunwar called the incident “politically motivated\" and said that the video was portrayed in a “wrong manner\". Kunwar said that the youth who was punished used to make videos of women while they were bathing. “So the villagers caught the accused and brought him to the panchayat, where the decision was taken in front of the gram sabha.\\n\\nSummary: ',\n",
       " 'A video purportedly showing former BJP MLA Devendra Kunwar from Jharkhand kicking a youth and making him lick his own spit for allegedly making videos of women while they were bathing has surfaced. As per reports, local police said no case has been registered in the matter so far. Several spectators had surrounded the youth.',\n",
       " 'Former Jharkhand BJP MLA Devendra Kunwar has been punished for allegedly making videos of women while bathing. The video shows the youth being kicked and spit and licking as he follows the instructions of the former legislator. The video has been shared online and has been shared by some people. Kunwar said the video was portrayed in a \"wrong manner\".')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[-200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6344e25b-2d2c-47f5-a1e8-ba7b43febc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Give a title to the given news article in not more than 15 words.\\n\\nA former BJP MLA from Jharkhand took law in his hands and decided to punish a youth for allegedly make videos of women while bathing. In the video, former Jarmundi MLA Devendra Kunwar can be seen telling the youth to do sit-ups. The leader then asks him to spit and lick. As he follows the instructions of the former legislator, Kunwar kicks him. The youth can be seen weeping in the video. The inhumane punishment was given at the ‘kangaroo court’ of Kunwar, during a gram sabha meeting. Some of the spectators could be seen cheering and recording the video. Reacting to the viral video, Kunwar called the incident “politically motivated\" and said that the video was portrayed in a “wrong manner\". Kunwar said that the youth who was punished used to make videos of women while they were bathing. “So the villagers caught the accused and brought him to the panchayat, where the decision was taken in front of the gram sabha.\\n\\nSummary: A video purportedly showing former BJP MLA Devendra Kunwar from Jharkhand kicking a youth and making him lick his own spit for allegedly making videos of women while they were bathing has surfaced. As per reports, local police said no case has been registered in the matter so far. Several spectators had surrounded the youth.\\n\\nTitle: ',\n",
       " \"J'khand ex-MLA kicks youth, makes him lick spit for taking videos of women bathing\",\n",
       " 'Video shows ex-BJP MLA Kunwar kicking youth for making videos of women bathing')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[-199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be6859e-138e-4fa5-8578-1e74d83b41d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-08-22 12:22:22.191544: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-22 12:22:23.686300: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-22 12:22:23.686423: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-22 12:22:23.686439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c46b001-5e6c-449b-8d2d-043c1ba76ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import cuda\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  \n",
    "torch.cuda.device_count()\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164eb30d-338b-4bdf-a459-48a8ba3bb098",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1810a31e-6eb0-41cb-a993-c1ab87a01482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = tokenizer.model_max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # [prompt, human text, model text]\n",
    "        inputs = self.tokenizer(self.data[index],             \n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                pad_to_max_length=True,\n",
    "                                return_token_type_ids=True,\n",
    "                                truncation=True,\n",
    "                                padding=\"max_length\")\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6fa56c1-6859-49b9-b79c-eb5441794c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train_size = int(len(training_data) * train_ratio)\n",
    "\n",
    "train_data = training_data[:train_size].copy()\n",
    "test_data = training_data[train_size:].copy()\n",
    "\n",
    "training_set = CustomDataset(train_data, tokenizer)\n",
    "testing_set = CustomDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f64d9c0-2f97-4724-bc3c-b1123875bf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([[13065,  3876,  1096,  ..., 50256, 50256, 50256],\n",
       "         [31056,    84,   311,  ..., 50256, 50256, 50256],\n",
       "         [40277,  6295,    84,  ..., 50256, 50256, 50256]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3c5fddc-e152-4371-be8c-632476d8d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 0\n",
    "                }\n",
    "\n",
    "trainloader = DataLoader(training_set, **train_params)\n",
    "testloader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80fe3468-1a0f-44e2-a2c5-725b113f60c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "781927bd-777c-429e-b00f-ba973cf08dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "class RewardModel(torch.nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.l1 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=8,\n",
    "            target_modules=[\"c_proj\", \"c_fc\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            # task_type=TaskType.SEQ_2_SEQ_LM\n",
    "        )\n",
    "\n",
    "        self.l1 = get_peft_model(self.l1, lora_config)\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(768, 1),\n",
    "        )\n",
    "        self.outl = nn.Sigmoid()\n",
    "        \n",
    "    def gpt2(self, ids, attention_mask, token_type_ids):\n",
    "        # logits shape: [batch=16, seqlen=1024, dim=768]\n",
    "        logits, _ = self.l1(ids, \n",
    "                         attention_mask=attention_mask,\n",
    "                         token_type_ids=token_type_ids,\n",
    "                         return_dict=False)\n",
    "        \n",
    "        sequence_lengths = (torch.eq(ids[:, 0], tokenizer.pad_token_id).long().argmax(-1) - 1).to(\n",
    "                    logits.device\n",
    "                )\n",
    "        \n",
    "        batch_size = ids.shape[0]\n",
    "        \n",
    "        # pooled logits shape: [batch=16, dim=768]\n",
    "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
    "        return pooled_logits\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        \"\"\"\n",
    "        ids shape = [batch, 3, 512]\n",
    "        3: prompt, human_text, model_text\n",
    "        \"\"\"  \n",
    "        prompt = self.gpt2(ids[:, 0], \n",
    "                         attention_mask=mask[:, 0], \n",
    "                         token_type_ids=token_type_ids[:, 0])\n",
    "        \n",
    "        human_text = self.gpt2(ids[:, 1], \n",
    "                             attention_mask=mask[:, 1], \n",
    "                             token_type_ids=token_type_ids[:, 1])\n",
    "            \n",
    "        model_text = self.gpt2(ids[:, 2], \n",
    "                             attention_mask=mask[:, 2], \n",
    "                             token_type_ids=token_type_ids[:, 2])\n",
    "\n",
    "        human_score = self.l2(prompt + human_text)\n",
    "        model_score = self.l2(prompt + model_text)\n",
    "        \n",
    "        return self.outl(human_score - model_score)\n",
    "    \n",
    "    def predict(self, ids, mask, token_type_ids):\n",
    "        \"\"\"\n",
    "        ids shape: [batch, 2, 512]\n",
    "        2: prompt, text\n",
    "        \"\"\"\n",
    "        prompt = self.gpt2(ids[:, 0], \n",
    "                           attention_mask=mask[:, 0], \n",
    "                           token_type_ids=token_type_ids[:, 0])\n",
    "        \n",
    "        text = self.gpt2(ids[:, 1], \n",
    "                         attention_mask=mask[:, 1], \n",
    "                         token_type_ids=token_type_ids[:, 1])\n",
    "        \n",
    "        return self.l2(prompt + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ae7408-d126-48d6-bb79-38bf05e762ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py:240: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "RewardModel                                                  [4, 1]                    --\n",
       "├─PeftModel: 1-1                                             [4, 1024, 768]            --\n",
       "│    └─LoraModel: 2-3                                        --                        (recursive)\n",
       "│    │    └─GPT2Model: 3-1                                   [4, 1024, 768]            125,324,544\n",
       "├─PeftModel: 1-2                                             [4, 1024, 768]            (recursive)\n",
       "│    └─LoraModel: 2-3                                        --                        (recursive)\n",
       "│    │    └─GPT2Model: 3-2                                   [4, 1024, 768]            (recursive)\n",
       "├─PeftModel: 1-3                                             [4, 1024, 768]            (recursive)\n",
       "│    └─LoraModel: 2-3                                        --                        (recursive)\n",
       "│    │    └─GPT2Model: 3-3                                   [4, 1024, 768]            (recursive)\n",
       "├─Sequential: 1-4                                            [4, 1]                    --\n",
       "│    └─Linear: 2-4                                           [4, 1]                    769\n",
       "├─Sequential: 1-5                                            [4, 1]                    (recursive)\n",
       "│    └─Linear: 2-5                                           [4, 1]                    (recursive)\n",
       "├─Sigmoid: 1-6                                               [4, 1]                    --\n",
       "==============================================================================================================\n",
       "Total params: 125,325,313\n",
       "Trainable params: 885,505\n",
       "Non-trainable params: 124,439,808\n",
       "Total mult-adds (G): 588.77\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.30\n",
       "Forward/backward pass size (MB): 10239.34\n",
       "Params size (MB): 246.28\n",
       "Estimated Total Size (MB): 10485.92\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model = RewardModel(dropout=0.01)\n",
    "model.to(device)\n",
    "\n",
    "# sample input\n",
    "sample_input = next(iter(trainloader))\n",
    "\n",
    "ids = sample_input[\"ids\"].to(device)\n",
    "mask = sample_input[\"mask\"].to(device)\n",
    "token_type_ids = sample_input[\"token_type_ids\"].to(device)\n",
    "\n",
    "summary(model, input_data=[ids, mask, token_type_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3154f425-9f23-4623-a0c3-965222248a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.AdamW(params=model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4661d05b-ece1-4611-93cc-aaf546c467e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    \n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader)\n",
    "    \n",
    "    for idx, data in enumerate(pbar):\n",
    "        ids = data[\"ids\"].to(device)\n",
    "        mask = data[\"mask\"].to(device)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(device)\n",
    "        \n",
    "        batch_size = ids.size()[0]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(ids, mask, token_type_ids)\n",
    "        target = torch.from_numpy(np.ones(shape=(batch_size, 1))).float().to(device)\n",
    "        \n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct += (\n",
    "            sum((pred.detach().squeeze(-1) >= 0.50).float() == target.squeeze(-1))\n",
    "            .detach()\n",
    "            .item()\n",
    "        )\n",
    "        processed += pred.detach().shape[0]\n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "        # tqdm writing\n",
    "        pbar.set_description(\n",
    "            desc=\"Train Epoch: {epoch}, Mini Batch: {batch}, Train Accuracy: {accuracy}, Train Loss: {loss}\".format(\n",
    "                epoch=epoch,\n",
    "                batch=idx+1,\n",
    "                accuracy=round((correct / processed) * 100, 4),\n",
    "                loss=round(total_loss / (idx+1), 4)\n",
    "            )\n",
    "        )\n",
    "    return round((correct / processed) * 100, 4), round(total_loss / (idx+1), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c7a050a-2527-428a-be9e-9fa0568785b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(pbar):\n",
    "            ids = data[\"ids\"].to(device)\n",
    "            mask = data[\"mask\"].to(device)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device)\n",
    "\n",
    "            batch_size = ids.size()[0]\n",
    "\n",
    "            pred = model(ids, mask, token_type_ids)\n",
    "            target = torch.from_numpy(np.ones(shape=(batch_size, 1))).float().to(device)\n",
    "\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            correct += (\n",
    "                sum((pred.detach().squeeze(-1) >= 0.50).float() == target.squeeze(-1))\n",
    "                .detach()\n",
    "                .item()\n",
    "            )\n",
    "            processed += pred.detach().shape[0]\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "            # tqdm writing\n",
    "            pbar.set_description(\n",
    "                desc=\"Test Epoch: {epoch}, Mini Batch: {batch}, Test Accuracy: {accuracy}, Test Loss: {loss}\".format(\n",
    "                    epoch=epoch,\n",
    "                    batch=idx+1,\n",
    "                    accuracy=round((correct / processed) * 100, 4),\n",
    "                    loss=round(total_loss / (idx+1), 4)\n",
    "                )\n",
    "            )\n",
    "        return round((correct / processed) * 100, 4), round(total_loss / (idx + 1), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59c79095-b019-443e-8c21-758be55e4f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 885505\n",
      "all model parameters: 125325313\n",
      "percentage of trainable model parameters: 0.71%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    \n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    \n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce495fd4-6c8f-4796-b692-3947e3ede4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Mini Batch: 1805, Train Accuracy: 50.5956, Train Loss: 0.8588:   9%|▊         | 1805/20632 [25:53<4:27:51,  1.17it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Train Epoch: 0, Mini Batch: 20632, Train Accuracy: 60.5797, Train Loss: 0.6812: 100%|██████████| 20632/20632 [4:54:56<00:00,  1.17it/s]  \n",
      "Test Epoch: 0, Mini Batch: 2107, Test Accuracy: 75.5102, Test Loss: 0.4982:  41%|████      | 2107/5158 [12:13<17:31,  2.90it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Test Epoch: 0, Mini Batch: 5158, Test Accuracy: 75.2957, Test Loss: 0.4957: 100%|██████████| 5158/5158 [29:56<00:00,  2.87it/s]\n",
      "Train Epoch: 1, Mini Batch: 20632, Train Accuracy: 67.4188, Train Loss: 0.5976: 100%|██████████| 20632/20632 [4:55:29<00:00,  1.16it/s]\n",
      "Test Epoch: 1, Mini Batch: 858, Test Accuracy: 76.6608, Test Loss: 0.4898:  17%|█▋        | 857/5158 [05:05<25:22,  2.82it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Test Epoch: 1, Mini Batch: 4364, Test Accuracy: 77.2399, Test Loss: 0.4939:  85%|████████▍ | 4363/5158 [25:51<04:45,  2.78it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Train Epoch: 2, Mini Batch: 20632, Train Accuracy: 69.2376, Train Loss: 0.5595: 100%|██████████| 20632/20632 [4:53:38<00:00,  1.17it/s]\n",
      "Test Epoch: 2, Mini Batch: 421, Test Accuracy: 77.8504, Test Loss: 0.4588:   8%|▊         | 420/5158 [02:25<28:20,  2.79it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Test Epoch: 2, Mini Batch: 5158, Test Accuracy: 77.9857, Test Loss: 0.4657: 100%|██████████| 5158/5158 [29:42<00:00,  2.89it/s]\n",
      "Train Epoch: 3, Mini Batch: 1177, Train Accuracy: 70.5395, Train Loss: 0.542:   6%|▌         | 1176/20632 [16:44<4:35:31,  1.18it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Train Epoch: 3, Mini Batch: 4735, Train Accuracy: 70.7181, Train Loss: 0.5414:  23%|██▎       | 4734/20632 [1:07:13<3:44:17,  1.18it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Train Epoch: 3, Mini Batch: 15558, Train Accuracy: 70.6871, Train Loss: 0.5434:  75%|███████▌  | 15558/20632 [3:40:23<1:11:28,  1.18it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Train Epoch: 3, Mini Batch: 16714, Train Accuracy: 70.764, Train Loss: 0.5441:  81%|████████  | 16714/20632 [3:56:45<55:13,  1.18it/s]   "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_accuracy, train_loss = train(epoch, model, trainloader, optimizer, criterion, device)\n",
    "    test_accuracy, test_loss = test(epoch, model, testloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b3f2a6d-e2c0-4705-bc2d-7b47dd96fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l1.save_pretrained(\"./reward_model_checkpoint/peft_gpt2/\")\n",
    "torch.save(model.l2.state_dict(), \"./reward_model_checkpoint/peft_gpt2/l2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fcde5-87bf-411a-867e-f99c6812b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PROMPT: EMBEDDING1\n",
    "HUMAN SUMMARY: EMBEDDING2\n",
    "MODEL GENERATED SUMMARY: EMBEDDING3\n",
    "\n",
    "SIGMOID(LINEAR(EMBEDDING1 + EMBEDDING2) - LINEAR(EMBEDDING1 + EMBEDDING3)) = 1\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
