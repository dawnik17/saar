{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53b3463-483d-4b00-9586-fac4f6a254c8",
   "metadata": {},
   "source": [
    "# **INSHORTS** (https://inshorts.me/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6383c9f-0d1c-4077-bb99-034d31677458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_data(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to fetch data. Status code:\", response.status_code)\n",
    "        data = dict()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data_with_count_trial(url):\n",
    "    counts = [100, 500] + [(i+1) * 1000 for i in range(10)]    \n",
    "    last_result = None\n",
    "    \n",
    "    for idx, count in tqdm(enumerate(counts)):\n",
    "        transit_url = url.format(count=count)\n",
    "        response = requests.get(transit_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            last_result = response.json()\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return last_result[\"data\"][\"articles\"]\n",
    "\n",
    "print_ = lambda x: print(x, \"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df1531d-bb7e-4d49-87fb-bd6a77256af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_url = \"https://inshorts.me/news/all?offset=0&limit={count}\"\n",
    "top_news_url = \"https://inshorts.me/news/top?offset=0&limit={count}\"\n",
    "trending_news_url = \"https://inshorts.me/news/trending?offset=0&limit={count}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e367e841-7641-4794-b2b1-d99680d25ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 unique dates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'09-10-23'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timestamp_to_date(timestamp_ms):\n",
    "    timestamp_seconds = timestamp_ms / 1000  # Convert milliseconds to seconds\n",
    "\n",
    "    # Convert timestamp to datetime object\n",
    "    date_object = datetime.fromtimestamp(timestamp_seconds)\n",
    "\n",
    "    # Format the datetime object as mm-dd-yy\n",
    "    formatted_date = date_object.strftime('%m-%d-%y')\n",
    "\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "dates = set()\n",
    "for url in [all_news_url, top_news_url, trending_news_url]:\n",
    "    timestamp = get_data(url.format(count=1))[\"data\"][\"articles\"][0][\"createdAt\"]\n",
    "    date = timestamp_to_date(timestamp)\n",
    "    dates.add(date)\n",
    "\n",
    "    \n",
    "print(f\"{len(dates)} unique dates\")\n",
    "date = dates.pop()\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467b6e6-08f8-4abc-8066-acd3d7da4ed6",
   "metadata": {},
   "source": [
    "# all news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714ef95b-3464-4d2a-ad5f-f52880346bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:25,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_news = get_data_with_count_trial(all_news_url)\n",
    "print_(len(all_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72c609-e516-4f33-9347-819130d3539a",
   "metadata": {},
   "source": [
    "# top news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1d4aa0-d6fe-4c6e-9133-059fe425a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:31,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3954 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_news = get_data_with_count_trial(top_news_url)\n",
    "print_(len(top_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fec77-80e5-4ed5-959c-55dd7b9f1514",
   "metadata": {},
   "source": [
    "# trending news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abab4a67-b944-4a0b-be10-8ce351534e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:21,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trending_news = get_data_with_count_trial(trending_news_url)\n",
    "print_(len(trending_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfaa429-d0de-4e9b-b732-df1f3422f55c",
   "metadata": {},
   "source": [
    "# news by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0afc57-18fc-48e9-af98-a6bf1b0b80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:19<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# the below 2 are to be used together\n",
    "get_all_topics = get_data(\"https://inshorts.me/news/topics\")\n",
    "topic_news_api = lambda topic: get_data(f\"https://inshorts.me/news/topics/{topic}\")[\"data\"][\"articles\"]\n",
    "\n",
    "# get topic names\n",
    "all_topics = [topic[\"topic\"] for topic in get_all_topics[\"data\"][\"topics\"]]\n",
    "\n",
    "# topic news\n",
    "topic_news = []\n",
    "for topic in tqdm(all_topics):\n",
    "    topic_news.extend(topic_news_api(topic))\n",
    "    \n",
    "print_(len(topic_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc80ed-7895-4237-bb38-d183be02fbe3",
   "metadata": {},
   "source": [
    "# news by query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dba903-8202-4a26-a4e2-8fca74ea9c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.14s/it]\u001b[A\n",
      "2it [00:18,  9.22s/it]\u001b[A\n",
      "  6%|▌         | 1/18 [00:18<05:13, 18.45s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.29s/it]\u001b[A\n",
      "2it [00:05,  3.29s/it]\u001b[A\n",
      "3it [00:25,  8.46s/it]\u001b[A\n",
      " 11%|█         | 2/18 [00:43<06:00, 22.54s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.83s/it]\u001b[A\n",
      "2it [00:21, 10.66s/it]\u001b[A\n",
      " 17%|█▋        | 3/18 [01:05<05:29, 21.99s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.21s/it]\u001b[A\n",
      "2it [00:08,  4.78s/it]\u001b[A\n",
      "3it [00:30, 10.23s/it]\u001b[A\n",
      " 22%|██▏       | 4/18 [01:35<05:56, 25.43s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.94s/it]\u001b[A\n",
      "2it [00:10,  5.85s/it]\u001b[A\n",
      "3it [00:32, 10.82s/it]\u001b[A\n",
      " 28%|██▊       | 5/18 [02:08<06:03, 27.97s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.02s/it]\u001b[A\n",
      "2it [00:11,  6.09s/it]\u001b[A\n",
      "3it [00:32, 10.85s/it]\u001b[A\n",
      " 33%|███▎      | 6/18 [02:40<05:54, 29.52s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.72s/it]\u001b[A\n",
      "2it [00:19,  9.94s/it]\u001b[A\n",
      " 39%|███▉      | 7/18 [03:00<04:50, 26.37s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.70s/it]\u001b[A\n",
      "2it [00:08,  4.68s/it]\u001b[A\n",
      "3it [00:28,  9.61s/it]\u001b[A\n",
      " 44%|████▍     | 8/18 [03:29<04:31, 27.16s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.71s/it]\u001b[A\n",
      "2it [00:09,  5.20s/it]\u001b[A\n",
      "3it [00:30, 10.13s/it]\u001b[A\n",
      " 50%|█████     | 9/18 [04:00<04:13, 28.18s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.60s/it]\u001b[A\n",
      "2it [00:10,  5.44s/it]\u001b[A\n",
      "3it [00:32, 10.72s/it]\u001b[A\n",
      " 56%|█████▌    | 10/18 [04:32<03:55, 29.41s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.45s/it]\u001b[A\n",
      "2it [00:23, 11.76s/it]\u001b[A\n",
      " 61%|██████    | 11/18 [04:55<03:13, 27.61s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.77s/it]\u001b[A\n",
      "2it [00:09,  5.04s/it]\u001b[A\n",
      "3it [00:29,  9.84s/it]\u001b[A\n",
      " 67%|██████▋   | 12/18 [05:25<02:49, 28.20s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.34s/it]\u001b[A\n",
      "2it [00:21, 10.99s/it]\u001b[A\n",
      " 72%|███████▏  | 13/18 [05:47<02:11, 26.32s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.52s/it]\u001b[A\n",
      "2it [00:18,  9.36s/it]\u001b[A\n",
      " 78%|███████▊  | 14/18 [06:05<01:36, 24.02s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.05s/it]\u001b[A\n",
      "2it [00:06,  3.73s/it]\u001b[A\n",
      "3it [00:25,  8.61s/it]\u001b[A\n",
      " 83%|████████▎ | 15/18 [06:31<01:13, 24.57s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.70s/it]\u001b[A\n",
      "2it [00:20, 10.46s/it]\u001b[A\n",
      " 89%|████████▉ | 16/18 [06:52<00:46, 23.48s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.45s/it]\u001b[A\n",
      "2it [00:08,  4.73s/it]\u001b[A\n",
      "3it [00:30, 10.07s/it]\u001b[A\n",
      " 94%|█████████▍| 17/18 [07:22<00:25, 25.51s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.42s/it]\u001b[A\n",
      "2it [00:06,  3.50s/it]\u001b[A\n",
      "3it [00:25,  8.54s/it]\u001b[A\n",
      "100%|██████████| 18/18 [07:48<00:00, 26.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8871 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search_news_api = lambda query: get_data_with_count_trial(\"https://inshorts.me/news/search?query={query}&offset=0&limit={{count}}\".format(query=query))\n",
    "\n",
    "# topic news\n",
    "search_news = []\n",
    "\n",
    "for topic in tqdm(all_topics):\n",
    "    search_news.extend(search_news_api(query=topic))\n",
    "    \n",
    "print_(len(search_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec4430b-bf03-479f-877c-1ae02002ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16183 news\n"
     ]
    }
   ],
   "source": [
    "clubbed_overall_news = []\n",
    "\n",
    "\n",
    "for news in all_news + top_news + trending_news + topic_news + search_news:\n",
    "    clubbed_overall_news.append(\n",
    "        {\"title\": news[\"title\"].strip(),\n",
    "         \"summary\": news[\"content\"].strip(),\n",
    "         \"link\": news[\"sourceUrl\"],\n",
    "         \"image_link\": news[\"imageUrl\"],\n",
    "         \"source\": \"inshorts\"}\n",
    "    )\n",
    "    \n",
    "print_(len(clubbed_overall_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b99e7d-449e-419f-8e9d-ed3cf0340e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"How is India-Middle East-Europe Corridor different from China's Belt & Road Initiative?\",\n",
       " 'summary': 'The India-Middle East-Europe Economic Corridor will be substantially different from China\\'s Belt and Road Initiative, Railways Minister Ashwini Vaishnaw said. Unlike the BRI, where a huge debt burden gets imposed on host nations, the G20 project will bring in revenue and be bankable, Vaishnaw added. \"The BRI came with...conditions...[Now] countries can decide on basis of its needs,\" he added.',\n",
       " 'link': 'https://www.news18.com/amp/videos/india/railway-minister-on-how-india-middle-east-eu-corridor-differs-from-china-s-bri-g20-summit-news18-8572165.html?utm_campaign=fullarticle&utm_medium=referral&utm_source=inshorts',\n",
       " 'image_link': 'https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2023/09_sep/10_sun/img_1694367018282_376.jpg?',\n",
       " 'source': 'inshorts'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clubbed_overall_news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51e21e-0ba3-4c4c-ab3e-d1441d63e7dd",
   "metadata": {},
   "source": [
    "# Deduplicate News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851a3ec5-ea31-4196-91ff-6cbdd38d4af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11024  unique links\n",
      "16183  total links\n"
     ]
    }
   ],
   "source": [
    "print(len({n[\"link\"] for n in clubbed_overall_news}), \" unique links\")\n",
    "print(len(clubbed_overall_news), \" total links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e7e36d-6edc-46bd-99cd-5113c2c649e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qblocks/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11135 news\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from saar.utils import deduplicate_list_of_dicts, get_full_news\n",
    "\n",
    "keys_to_check = ['title', 'summary', 'link']\n",
    "clubbed_overall_news = deduplicate_list_of_dicts(clubbed_overall_news, keys_to_check)\n",
    "\n",
    "print_(len(clubbed_overall_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff5fb6-ff99-4316-bb6c-610e19f3f06d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **RSS FEEDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dc45cc4-5765-49f2-9d15-a190e5256166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rss_feeds\n",
    "# import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bf83776-23fe-47bf-b7ec-012a12574cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [02:52<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3851 news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# feed_urls = list(set(rss_feeds.rss_feeds))\n",
    "# feed_news = []\n",
    "\n",
    "# for feed_url in tqdm(feed_urls):\n",
    "#     feed = feedparser.parse(feed_url)\n",
    "    \n",
    "#     # Iterate through the entries in the feed\n",
    "#     for entry in feed.entries:\n",
    "#         try:\n",
    "#             feed_news.append(\n",
    "#                 {\"title\": entry.title,\n",
    "#                  \"summary\": entry.summary,\n",
    "#                  \"link\": entry.link,\n",
    "#                  \"source\": \"rss feed\"}\n",
    "#             )\n",
    "#         except:\n",
    "#             continue\n",
    "            \n",
    "# print_(len(feed_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f95a71f8-65df-44b6-9707-70f535707a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Families of babies murdered by nurse Lucy Letby vow to continue their search for answers',\n",
       " 'summary': 'The families of babies murdered by Lucy Letby have vowed to continue their search for answers as questions swirled around what more could have been done to stop her killing spree.',\n",
       " 'link': 'https://news.sky.com/story/families-of-babies-murdered-by-lucy-letby-vow-to-continue-their-search-for-answers-12942744',\n",
       " 'source': 'rss feed'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed_news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cb7d5-8cc3-45d3-8bcf-3680e051dba5",
   "metadata": {},
   "source": [
    "# **AGGREGATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6558d656-8f37-4257-b536-20d5c67e5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = clubbed_overall_news # + feed_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER OUT NEWS THAT WE ALREADY HAVE \n",
    "from os.path import join\n",
    "import json\n",
    "\n",
    "folder_path = \"../../../data/training/\"\n",
    "path = join(folder_path, \"inshorts.json\")\n",
    "\n",
    "with open(path, 'r') as json_file:\n",
    "    existing_data = json.load(json_file)\n",
    "                \n",
    "# Create a set of tuples containing (key1, key2) from data for faster look-up\n",
    "# Filter news based on the presence of (link, summary) in data\n",
    "set_data_keys = {(d['link'], d['summary']) for d in existing_data}\n",
    "news = [d for d in news if (d['link'], d['summary']) not in set_data_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c685d-ee4c-45f9-8371-52ec0b7f2058",
   "metadata": {},
   "source": [
    "# Full Text from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "078b335d-9d1c-4135-8a60-7cc5101e3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get full news\n",
    "\"\"\"\n",
    "news = [get_full_news(new) for new in tqdm(news)]\n",
    "news = [new for new in news if new is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89049a04-47ec-4ef0-95ec-74866098723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint\n",
    "import os\n",
    "import json\n",
    "\n",
    "path = join(folder_path, f\"{date}.json\")\n",
    "\n",
    "with open(path, 'w') as json_file:\n",
    "    json.dump(news, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4516b1-a640-47bd-9f1a-57545f2a3cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-09-11 07:43:44.390496: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 07:43:45.931315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-11 07:43:45.931434: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-11 07:43:45.931449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "[nltk_data] Downloading package punkt to /home/qblocks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from saar.infer import Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3286b556-e499-4955-b105-499f80bdf794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7849"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494d8edb-d519-4602-81d6-b0578e25377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "run inference\n",
    "\"\"\"\n",
    "if len(news) > 0:\n",
    "    # summary adapter, title adapter path\n",
    "    summary_adapter_path = os.environ[\"SUMMARY_ADAPTER_PATH\"]\n",
    "    title_adapter_path = os.environ[\"TITLE_ADAPTER_PATH\"]\n",
    "\n",
    "    logging.info(\"loading models..\")\n",
    "    infer = Infer(\n",
    "        summary_adapter_path=summary_adapter_path, title_adapter_path=title_adapter_path\n",
    "    )\n",
    "\n",
    "    # batching\n",
    "    batch = lambda data, batch_size: [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    \n",
    "    batch_size = 60 # int(os.environ[\"BATCH_SIZE\"])\n",
    "    news = batch(news, batch_size=batch_size)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    logging.info(\"running inference..\")\n",
    "    for news_batch in tqdm(news):\n",
    "        # generate summary\n",
    "        news_batch = infer(mode=\"summary\", data=news_batch)\n",
    "\n",
    "        # generate title\n",
    "        # NOTE: Title generation needs summary already generated as \"generated_summary\" key in the news dict\n",
    "        news_batch = infer(mode=\"title\", data=news_batch)\n",
    "        data.extend(news_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff8ff88-a0e1-4e50-b3fc-701917246c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7849, 131)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b279d",
   "metadata": {},
   "source": [
    "# FOR SOME REASON, THIS CODE DOESN'T PRODUCE DIFFERENT TEXT EVERYTIME IT RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3b193-af1c-492e-80dc-eaa909d11aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/131 [02:51<6:12:17, 171.83s/it]"
     ]
    }
   ],
   "source": [
    "# sample multiple outputs for one news article from the model\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "infer.summary_generation_config = GenerationConfig(\n",
    "            max_new_tokens=200,\n",
    "            num_beams=8,\n",
    "            do_sample=True,\n",
    "            temperature=2.,\n",
    "            top_k=30,\n",
    "            top_p=0.8,\n",
    "        )\n",
    "\n",
    "infer.title_generation_config = GenerationConfig(\n",
    "            max_new_tokens=25,\n",
    "            num_beams=8,\n",
    "            do_sample=True,\n",
    "            temperature=2.,\n",
    "            top_k=30,\n",
    "            top_p=0.8,\n",
    "        )\n",
    "\n",
    "num_of_model_output_samples = 5\n",
    "\n",
    "for news_batch in tqdm(news):\n",
    "    for _ in range(num_of_model_output_samples):\n",
    "        # generate summary\n",
    "        news_batch = infer(mode=\"summary\", data=news_batch)\n",
    "\n",
    "        # generate title\n",
    "        # NOTE: Title generation needs summary already generated as \"generated_summary\" key in the news dict\n",
    "        news_batch = infer(mode=\"title\", data=news_batch)\n",
    "        data.extend(news_batch.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1ddea-a802-4792-a094-98f06f1d18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate\n",
    "data = deduplicate_list_of_dicts(data, [\"link\", \"generated_summary\", \"generated_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a41fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_data.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d954051-d295-4a79-a546-4d7afa703429",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'w') as json_file:\n",
    "    json.dump(existing_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ca5d9-c038-4a71-a3fb-37ff010a87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
